"""åœ¨çº¿è®­ç»ƒä¸»å¾ªç¯ - AlphaZeroå¼ç¨‹åºåˆæˆ

ä»é›¶å¼€å§‹è®­ç»ƒï¼šNNéšæœºåˆå§‹åŒ– â†’ MCTSæœç´¢ â†’ æ”¶é›†æ ·æœ¬ â†’ æ›´æ–°NN â†’ å¾ªç¯
"""
from __future__ import annotations

# ã€ä¿®å¤Python 3.13å…¼å®¹æ€§ã€‘ç¦ç”¨PyTorchç¼–è¯‘åŠŸèƒ½
import os
os.environ['PYTORCH_JIT'] = '0'
os.environ['TORCH_COMPILE_DISABLE'] = '1'

import argparse, time, json, random
from typing import List, Dict, Any, Tuple, Optional
from collections import deque
import numpy as np

# å¯¼å…¥ç°æœ‰æ¨¡å— - ç®€åŒ–å¯¼å…¥,åªå¯¼å…¥å¿…éœ€ç»„ä»¶
import sys, pathlib
_SCRIPT_DIR = pathlib.Path(__file__).resolve().parent
_PKG_ROOT = _SCRIPT_DIR.parent
if str(_PKG_ROOT) not in sys.path:
    sys.path.insert(0, str(_PKG_ROOT))
if str(_SCRIPT_DIR) not in sys.path:
    sys.path.insert(0, str(_SCRIPT_DIR))

# Ensure Isaac Gym python bindings are importable (repo vendor path)
try:
    _REPO_ROOT = _PKG_ROOT.parent
    _GYM_PY = _REPO_ROOT / 'isaacgym' / 'python'
    if _GYM_PY.exists() and str(_GYM_PY) not in sys.path:
        sys.path.insert(0, str(_GYM_PY))
    # æå‰å¯¼å…¥ isaacgymï¼Œç¡®ä¿å…¶å…ˆäº torch å¯¼å…¥
    try:
        from isaacgym import gymapi  # type: ignore
    except Exception:
        pass
except Exception:
    pass

# ç›´æ¥å¯¼å…¥å¿…éœ€æ¨¡å—ï¼ˆé¿å…å¾ªç¯ä¾èµ–ï¼‰
from mcts_training.mcts import MCTS_Agent, MCTSNode
from mcts_training.policy.policy_nn import PolicyValueNNLarge, EDIT_TYPES
from mcts_training.program_features import featurize_program

# GNNç›¸å…³æ¨¡å—ï¼ˆå¯é€‰ï¼‰
try:
    from gnn_features import ast_to_pyg_graph, batch_programs_to_graphs
    from gnn_policy_nn import GNNPolicyValueNet as GNNPolicyValueNetV1
    # v2 å¯é€‰å¯¼å…¥
    try:
        from gnn_policy_nn_v2 import create_gnn_policy_value_net_v2 as create_gnn_policy_value_net_v2
        GNN_V2_AVAILABLE = True
    except ImportError:
        create_gnn_policy_value_net_v2 = None  # type: ignore
        GNN_V2_AVAILABLE = False
    from torch_geometric.data import Batch as PyGBatch
    GNN_AVAILABLE = True
except ImportError as e:
    print(f"[Warning] GNNæ¨¡å—ä¸å¯ç”¨: {e}")
    GNN_AVAILABLE = False
    GNN_V2_AVAILABLE = False
    ast_to_pyg_graph = None
    batch_programs_to_graphs = None
    GNNPolicyValueNetV1 = None  # type: ignore
    create_gnn_policy_value_net_v2 = None  # type: ignore
    PyGBatch = None

# å¯¼å…¥batch_evaluationï¼ˆå¯èƒ½éœ€è¦Isaac Gymï¼‰ï¼›ç¡®ä¿åœ¨å¯¼å…¥ torch ä¹‹å‰å°è¯•å¯¼å…¥ isaacgym
try:
    from batch_evaluation import BatchEvaluator
    BATCH_EVAL_AVAILABLE = True
except Exception as e:
    print(f"[Warning] BatchEvaluatorä¸å¯ç”¨: {e}")
    BATCH_EVAL_AVAILABLE = False
    BatchEvaluator = None  # type: ignore

# ç°åœ¨å†å¯¼å…¥ torch åŠå…¶å­æ¨¡å—ï¼Œé¿å…ç ´å isaacgym çš„å¯¼å…¥é¡ºåºè¦æ±‚
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim

# æ— Isaac Gymæ—¶çš„ç®€æ˜“å ä½Evaluatorï¼ˆä»…ç”¨äºå¼€å‘/å•å…ƒæµ‹è¯•ï¼Œä¸ä»£è¡¨çœŸå®æ€§èƒ½ï¼‰
class _DummyEvaluator:
    def __init__(self, *args, **kwargs) -> None:
        self._rng = random.Random(0)
    def evaluate_single(self, program: List[Dict[str, Any]]) -> float:
        # ç²—ç•¥æŒ‰è§„åˆ™æ•°ç»™ä¸€ç‚¹åå¥½ï¼Œä»ä¿ç•™éšæœºæ€§ï¼Œä¾¿äºè·‘é€šæµç¨‹
        base = float(len(program)) * 0.05
        return base + (self._rng.random() - 0.5) * 0.1
    def evaluate_batch(self, programs: List[List[Dict[str, Any]]]):
        return [self.evaluate_single(p) for p in programs]

# å¯¼å…¥serialization
try:
    from serialization import save_program_json as _save_prog
    def save_program_json(program, path):  # type: ignore
        _save_prog(program, path)
except Exception:
    def save_program_json(program, path):  # type: ignore
        import json
        # ç®€åŒ–ç‰ˆä¿å­˜ï¼ˆä¸åŒ…å«èŠ‚ç‚¹å¯¹è±¡ï¼‰
        simplified = []
        for rule in program:
            simple_rule = {
                'name': rule.get('name', 'rule'),
                'multiplier': rule.get('multiplier', [1.0, 1.0, 1.0])
            }
            simplified.append(simple_rule)
        
        with open(path, 'w') as f:
            json.dump({'rules': simplified, 'note': 'Simplified format'}, f, indent=2)


class ReplayBuffer:
    """ç»éªŒå›æ”¾ç¼“å†²åŒºï¼ˆæ”¯æŒå›ºå®šç‰¹å¾å’ŒGNNå›¾æ•°æ®ï¼‰"""
    
    def __init__(self, capacity: int = 50000, use_gnn: bool = False):
        self.capacity = capacity
        self.use_gnn = use_gnn
        self.buffer = deque(maxlen=capacity)
    
    def push(self, sample: Dict[str, Any]):
        """æ·»åŠ æ ·æœ¬
        
        å›ºå®šç‰¹å¾æ¨¡å¼: sample = {'features': tensor, 'policy_target': tensor, 'value_target': tensor}
        GNNæ¨¡å¼: sample = {'graph': PyG Data, 'policy_target': tensor, 'value_target': tensor}
        """
        self.buffer.append(sample)
    
    def sample(self, batch_size: int) -> List[Dict[str, Any]]:
        """éšæœºé‡‡æ ·"""
        return random.sample(self.buffer, min(batch_size, len(self.buffer)))
    
    def __len__(self):
        return len(self.buffer)


class OnlineTrainer:
    """åœ¨çº¿è®­ç»ƒå™¨ - AlphaZeroèŒƒå¼"""
    
    def __init__(self, args):
        self.args = args
        
        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"[Trainer] ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨GNN
        self.use_gnn = getattr(args, 'use_gnn', False) and GNN_AVAILABLE
        if args.use_gnn and not GNN_AVAILABLE:
            print("[Warning] --use-gnn æŒ‡å®šä½†GNNæ¨¡å—ä¸å¯ç”¨ï¼Œå›é€€åˆ°å›ºå®šç‰¹å¾ç½‘ç»œ")
            self.use_gnn = False
        
        # åˆå§‹åŒ–NNï¼ˆæ ¹æ®use_gnné€‰æ‹©æ¨¡å‹ï¼‰
        if self.use_gnn:
            # é€‰æ‹©ç‰ˆæœ¬
            nn_version = getattr(args, 'nn_version', 'v1')
            if nn_version == 'v2' and GNN_V2_AVAILABLE:
                print(f"[Trainer] ä½¿ç”¨ GNN v2 (Hierarchical Dual) ç½‘ç»œ")
                self.nn_model = create_gnn_policy_value_net_v2(
                    node_feature_dim=24,
                    policy_output_dim=len(EDIT_TYPES),
                    structure_hidden=256,
                    structure_layers=5,
                    structure_heads=8,
                    feature_layers=3,
                    feature_heads=8,
                    dropout=0.1
                ).to(self.device)
            else:
                if nn_version == 'v2' and not GNN_V2_AVAILABLE:
                    print("[Trainer] è¯·æ±‚ v2 ä½†æœªæ‰¾åˆ°æ¨¡å—ï¼Œå›é€€åˆ° v1")
                else:
                    print(f"[Trainer] ä½¿ç”¨ GNN v1 ç½‘ç»œ")
                self.nn_model = GNNPolicyValueNetV1(
                    node_feature_dim=24,
                    hidden_dim=args.nn_hidden,
                    num_layers=3,
                    num_heads=4,
                    policy_output_dim=len(EDIT_TYPES),
                    dropout=0.1
                ).to(self.device)
        else:
            print(f"[Trainer] ä½¿ç”¨å›ºå®šç‰¹å¾ç­–ç•¥-ä»·å€¼ç½‘ç»œ")
            self.nn_model = PolicyValueNNLarge(
                in_dim=64,
                hidden=args.nn_hidden,
                out_dim=len(EDIT_TYPES)
            ).to(self.device)
        
        # ç¦ç”¨torch compileé¿å…Python 3.13å…¼å®¹æ€§é—®é¢˜
        try:
            import os
            os.environ['PYTORCH_JIT'] = '0'
            os.environ['TORCH_COMPILE_DISABLE'] = '1'
        except Exception:
            pass
        
        try:
            self.optimizer = optim.Adam(
                self.nn_model.parameters(),
                lr=args.learning_rate,
                weight_decay=1e-4
            )
        except KeyboardInterrupt:
            raise
        except Exception as e:
            # å¦‚æœæ ‡å‡†Adamå¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨åˆ›å»º
            print(f"[Warning] Adamåˆå§‹åŒ–å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆ: {e}")
            self.optimizer = optim.SGD(
                self.nn_model.parameters(),
                lr=args.learning_rate,
                momentum=0.9
            )
        
        print(f"[Trainer] NNåˆå§‹åŒ–å®Œæˆ (å‚æ•°: {sum(p.numel() for p in self.nn_model.parameters())})")
        
        # ç»éªŒå›æ”¾
        self.replay_buffer = ReplayBuffer(capacity=args.replay_capacity, use_gnn=self.use_gnn)
        
        # è¯„ä¼°å™¨ï¼šæ”¯æŒå¼ºåˆ¶ä½¿ç”¨ Dummyï¼Œç”¨äºå¿«é€ŸA/BåŸºå‡†
        force_dummy = getattr(args, 'use_dummy_eval', False)
        if force_dummy or BatchEvaluator is None:
            if not force_dummy:
                print("[Trainer] ä½¿ç”¨ DummyEvaluatorï¼ˆæœªæ£€æµ‹åˆ° Isaac Gymï¼‰")
            else:
                print("[Trainer] å¼ºåˆ¶ä½¿ç”¨ DummyEvaluatorï¼ˆA/Bå¿«é€ŸåŸºå‡†ï¼‰")
            self.evaluator = _DummyEvaluator()
        else:
            self.evaluator = BatchEvaluator(
                trajectory_config=self._build_trajectory(),
                duration=args.duration,
                isaac_num_envs=args.isaac_num_envs,
                device=str(self.device),
                replicas_per_program=getattr(args, 'eval_replicas_per_program', 1),
                min_steps_frac=getattr(args, 'min_steps_frac', 0.0),
                reward_reduction=getattr(args, 'reward_reduction', 'sum'),
                strict_no_prior=False,  # âœ… å…è®¸ä½¿ç”¨çŠ¶æ€å˜é‡è¿›è¡Œåé¦ˆæ§åˆ¶!
                zero_action_penalty=1.5,
                use_fast_path=getattr(args, 'use_fast_path', False)
            )
        
        # ç»Ÿè®¡
        self.iteration = 0
        self.best_reward = -float('inf')
        self.best_program = None
        self.training_stats = []
        self._mcts_stats = {}  # MCTSæ€§èƒ½ç»Ÿè®¡
    
    def _build_trajectory(self) -> Dict[str, Any]:
        """æ„å»ºè½¨è¿¹é…ç½®"""
        if self.args.traj == 'hover':
            return {'type': 'hover', 'initial_xyz': [0, 0, 1.0], 'params': {}}
        elif self.args.traj == 'figure8':
            return {'type': 'figure8', 'initial_xyz': [0, 0, 1.0], 'params': {'A': 0.8, 'B': 0.5, 'period': 12}}
        elif self.args.traj == 'circle':
            return {'type': 'circle', 'initial_xyz': [0, 0, 0.8], 'params': {'R': 0.9, 'period': 10}}
        elif self.args.traj == 'helix':
            return {'type': 'helix', 'initial_xyz': [0, 0, 0.5], 'params': {'R': 0.7, 'period': 10, 'v_z': 0.15}}
        else:
            raise ValueError(f"Unknown trajectory: {self.args.traj}")
    
    def _generate_random_program(self) -> List[Dict[str, Any]]:
        """ç”Ÿæˆéšæœºåˆå§‹ç¨‹åº"""
        # ä½¿ç”¨MCTSçš„éšæœºç”Ÿæˆé€»è¾‘
        mcts = MCTS_Agent(
            evaluation_function=lambda p: 0.0,  # å ä½ç¬¦
            dsl_variables=['pos_err', 'vel_err'],
            dsl_constants=[0.0, 1.0],
            dsl_operators=['+', '-', '*']
        )
        return mcts._generate_random_segmented_program()
    
    def _load_program_from_json(self, path: str) -> Optional[List[Dict[str, Any]]]:
        """ä» JSON æ–‡ä»¶åŠ è½½ç¨‹åºï¼ˆç”¨äº warm startï¼‰"""
        try:
            import json
            with open(path, 'r') as f:
                data = json.load(f)
            
            # å°è¯•æå– rules å­—æ®µ
            if isinstance(data, dict) and 'rules' in data:
                rules = data['rules']
            elif isinstance(data, list):
                rules = data
            else:
                print(f"[Warning] æ— æ³•è§£æç¨‹åºæ–‡ä»¶æ ¼å¼: {path}")
                return None
            
            # ç®€å•éªŒè¯
            if not isinstance(rules, list) or len(rules) == 0:
                print(f"[Warning] ç¨‹åºæ–‡ä»¶ä¸ºç©ºæˆ–æ ¼å¼é”™è¯¯: {path}")
                return None
            
            print(f"[Trainer] âœ… ä» {path} åŠ è½½äº† {len(rules)} æ¡è§„åˆ™")
            return rules
            
        except FileNotFoundError:
            print(f"[Warning] ç¨‹åºæ–‡ä»¶ä¸å­˜åœ¨: {path}")
            return None
        except Exception as e:
            print(f"[Warning] åŠ è½½ç¨‹åºæ–‡ä»¶å¤±è´¥: {e}")
            return None
    
    def mcts_search(self, root_program: List[Dict[str, Any]], num_simulations: int = 800) -> Tuple[List[Any], List[int]]:
        """
        æ‰§è¡ŒMCTSæœç´¢ï¼ˆä½¿ç”¨å½“å‰NNå¼•å¯¼ï¼‰
        
        Returns:
            children: æ‰€æœ‰å­èŠ‚ç‚¹
            visit_counts: è®¿é—®æ¬¡æ•°åˆ†å¸ƒ
        """
        # åˆ›å»ºMCTS agent
        mcts = MCTS_Agent(
            evaluation_function=self.evaluator.evaluate_single,
            # ä½¿ç”¨åº•å±‚çŠ¶æ€å˜é‡ï¼Œæå‡è¡¨è¾¾åŠ›ï¼ˆä¸¥æ ¼é›¶å…ˆéªŒï¼Œä¸å¼•å…¥ PID å¢ç›Šè¯­ä¹‰ï¼‰
            dsl_variables=[
                'pos_err_x','pos_err_y','pos_err_z','pos_err_xy','pos_err_z_abs',
                'vel_x','vel_y','vel_z','vel_err',
                'ang_vel_x','ang_vel_y','ang_vel_z','ang_vel','ang_vel_mag',
                'err_i_x','err_i_y','err_i_z',
                'err_p_roll','err_p_pitch','err_p_yaw','rpy_err_mag',
                'err_d_x','err_d_y','err_d_z','err_d_roll','err_d_pitch','err_d_yaw'
            ],
            # å¸¸æ•°åŸºæ•°æ›´ç»†ï¼Œåˆ©äºæ•°å€¼ç¼©æ”¾
            dsl_constants=[0.0, 0.05, 0.1, 0.3, 0.5, 1.0, 2.0],
            # è¡¨è¾¾å¼ç®—å­ï¼šä¿ç•™åŸºç¡€ä»£æ•° + ç®€å•å¹…åº¦å‹ç¼©ï¼Œä¸åœ¨æ¡ä»¶ä¸­å¼•å…¥ä¸‰è§’ï¼ˆæ¡ä»¶ç”Ÿæˆå™¨å·²æœ‰å®‰å…¨çº¦æŸï¼‰
            dsl_operators=['+','-','*','/','max','min','abs','sqrt','log1p','>','<'],
            exploration_weight=self.args.exploration_weight,
            max_depth=self.args.max_depth
        )
        
        # è®¾ç½®root
        root = MCTSNode(root_program, parent=None, depth=0)
        mcts.root = root
        
        # ğŸ”§ ä¼˜åŒ–1: GNNå…ˆéªŒç¼“å­˜ (é¿å…é‡å¤æ¨ç†)
        gnn_prior_cache = {}  # prog_hash -> (prior_p, value_estimate)
        
        def get_program_hash(program):
            """ç”Ÿæˆç¨‹åºçš„å“ˆå¸Œå€¼ç”¨äºç¼“å­˜ï¼ˆä½¿ç”¨ç¨‹åºé•¿åº¦+å­—ç¬¦ä¸²è¡¨ç¤ºï¼‰"""
            try:
                # ç®€å•ä½†æœ‰æ•ˆçš„å“ˆå¸Œ: ç¨‹åºé•¿åº¦ + è§„åˆ™æ•° + å­—ç¬¦ä¸²è¡¨ç¤ºçš„å“ˆå¸Œ
                prog_str = str(program)
                return hash((len(program), prog_str))
            except:
                # å›é€€ï¼šä½¿ç”¨idï¼ˆä¸ç¼“å­˜ï¼‰
                return id(program)
        
        # ğŸ”§ ä¼˜åŒ–2: æ‰¹é‡GNNæ¨ç†ç¼“å†²åŒº
        pending_gnn_nodes = []  # æ”¶é›†éœ€è¦GNNæ¨ç†çš„æ–°èŠ‚ç‚¹
        
        # ğŸ”§ æ‰¹é‡è¯„ä¼°ä¼˜åŒ–ï¼šæ”¶é›†å¾…è¯„ä¼°çš„leaf nodes
        pending_evals = []  # [(leaf, path, use_real_sim)]
        
        # æ‰§è¡ŒMCTSæ¨¡æ‹Ÿï¼ˆåªåšæ ‘æ‰©å±•ï¼Œå»¶è¿ŸGNNæ¨ç†ï¼‰
        for sim_idx in range(num_simulations):
            # Selection + Expansionï¼ˆä½¿ç”¨NNå…ˆéªŒï¼‰
            node = root
            path = [node]
            
            # Selectioné˜¶æ®µ
            while node.children and not node.is_fully_expanded():
                # ä½¿ç”¨PUCTé€‰æ‹©ï¼ˆé›†æˆNNå…ˆéªŒï¼‰
                node = self._select_child_puct(node)
                path.append(node)
            
            # Expansioné˜¶æ®µ
            if not node.is_fully_expanded():
                # ç”Ÿæˆæ–°å­èŠ‚ç‚¹ï¼Œåˆ†é…NNå…ˆéªŒ
                mcts._ensure_mutations(node)
                
                if node.untried_mutations and len(node.expanded_actions) < len(node.untried_mutations):
                    # é€‰æ‹©ä¸€ä¸ªæœªæ‰©å±•çš„å˜å¼‚
                    unexpanded_idx = [i for i in range(len(node.untried_mutations)) 
                                     if i not in node.expanded_actions][0]
                    mutation = node.untried_mutations[unexpanded_idx]
                    
                    # å…‹éš†ç¨‹åºå¹¶åº”ç”¨å˜å¼‚
                    child_program = [mcts._clone_rule(r) for r in node.program]
                    mcts._apply_mutation(child_program, mutation)
                    
                    # åˆ›å»ºå­èŠ‚ç‚¹
                    child = MCTSNode(child_program, parent=node, depth=node.depth + 1)
                    edit_type = mutation[0]
                    child._edit_type = edit_type
                    
                    # ğŸš€ ä¼˜åŒ–: æ£€æŸ¥ç¼“å­˜
                    prog_hash = get_program_hash(child_program)
                    if prog_hash in gnn_prior_cache:
                        # å‘½ä¸­ç¼“å­˜ï¼Œç›´æ¥ä½¿ç”¨
                        child._prior_p, child._cached_value = gnn_prior_cache[prog_hash]
                    else:
                        # æœªå‘½ä¸­ï¼ŒåŠ å…¥æ‰¹é‡æ¨ç†é˜Ÿåˆ—
                        child._prior_p = 1.0 / len(EDIT_TYPES)  # é»˜è®¤å…ˆéªŒ
                        child._cached_value = None
                        child._prog_hash = prog_hash
                        pending_gnn_nodes.append((child, edit_type))
                    
                    node.children.append(child)
                    node.expanded_actions.add(unexpanded_idx)
                    path.append(child)
            
            # ğŸ”§ æ”¶é›†leafå¾…æ‰¹é‡è¯„ä¼°ï¼ˆä¸ç«‹å³è¯„ä¼°ï¼‰
            leaf = path[-1]
            use_real_sim = random.random() < getattr(self.args, 'real_sim_frac', 0.8)
            pending_evals.append((leaf, path, use_real_sim))
        
        # ğŸš€ æ‰¹é‡GNNæ¨ç†é˜¶æ®µ (ä¸€æ¬¡æ¨ç†æ‰€æœ‰æ–°èŠ‚ç‚¹)
        if pending_gnn_nodes:
            try:
                with torch.no_grad():
                    if self.use_gnn:
                        # æ‰¹é‡æ„å»ºå›¾
                        graphs = [ast_to_pyg_graph(child.program) for child, _ in pending_gnn_nodes]
                        from torch_geometric.data import Batch
                        batch_graph = Batch.from_data_list(graphs).to(self.device)
                        policy_logits, value_preds = self.nn_model(batch_graph)
                    else:
                        # æ‰¹é‡ç‰¹å¾åŒ–
                        features = torch.stack([featurize_program(child.program) 
                                               for child, _ in pending_gnn_nodes]).to(self.device)
                        policy_logits, value_preds = self.nn_model(features)
                    
                    # åˆ†é…å…ˆéªŒå’Œç¼“å­˜
                    policy_probs = F.softmax(policy_logits, dim=-1)
                    for idx, (child, edit_type) in enumerate(pending_gnn_nodes):
                        if edit_type in EDIT_TYPES:
                            type_idx = EDIT_TYPES.index(edit_type)
                            prior_p = policy_probs[idx, type_idx].item()
                        else:
                            prior_p = 1.0 / len(EDIT_TYPES)
                        
                        value_est = value_preds[idx].item() if value_preds.dim() > 0 else value_preds.item()
                        child._prior_p = prior_p
                        child._cached_value = value_est
                        
                        # æ›´æ–°ç¼“å­˜
                        if hasattr(child, '_prog_hash'):
                            gnn_prior_cache[child._prog_hash] = (prior_p, value_est)
            except Exception as e:
                # æ‰¹é‡æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                for child, _ in pending_gnn_nodes:
                    child._prior_p = 1.0 / len(EDIT_TYPES)
                    child._cached_value = None
        
        # ğŸ”§ æ‰¹é‡è¯„ä¼°é˜¶æ®µ
        # åˆ†ç¦»çœŸå®ä»¿çœŸå’ŒNNä¼°å€¼
        real_sim_leaves = [(leaf, path) for leaf, path, use_real in pending_evals if use_real]
        nn_sim_leaves = [(leaf, path) for leaf, path, use_real in pending_evals if not use_real]
        
        # æ‰¹é‡çœŸå®ä»¿çœŸ
        if real_sim_leaves:
            programs = [leaf.program for leaf, _ in real_sim_leaves]
            rewards = self.evaluator.evaluate_batch(programs)
            for (leaf, path), reward in zip(real_sim_leaves, rewards):
                for node in reversed(path):
                    node.visits += 1
                    node.value_sum += reward
        
        # ğŸš€ æ‰¹é‡NNä¼°å€¼ (ä½¿ç”¨ç¼“å­˜ + æ‰¹é‡æ¨ç†)
        if nn_sim_leaves:
            # æ£€æŸ¥å“ªäº›å·²æœ‰ç¼“å­˜å€¼
            cached_leaves = []
            uncached_leaves = []
            for leaf, path in nn_sim_leaves:
                if hasattr(leaf, '_cached_value') and leaf._cached_value is not None:
                    # ä½¿ç”¨ç¼“å­˜çš„value
                    cached_leaves.append((leaf, path, leaf._cached_value * 10.0))
                else:
                    # éœ€è¦æ‰¹é‡æ¨ç†
                    prog_hash = get_program_hash(leaf.program)
                    if prog_hash in gnn_prior_cache:
                        _, value_est = gnn_prior_cache[prog_hash]
                        cached_leaves.append((leaf, path, value_est * 10.0))
                    else:
                        uncached_leaves.append((leaf, path))
            
            # å¤„ç†ç¼“å­˜å‘½ä¸­çš„
            for leaf, path, reward in cached_leaves:
                for node in reversed(path):
                    node.visits += 1
                    node.value_sum += reward
            
            # æ‰¹é‡æ¨ç†æœªç¼“å­˜çš„
            if uncached_leaves:
                try:
                    with torch.no_grad():
                        if self.use_gnn:
                            graphs = [ast_to_pyg_graph(leaf.program) for leaf, _ in uncached_leaves]
                            from torch_geometric.data import Batch
                            batch_graph = Batch.from_data_list(graphs).to(self.device)
                            _, value_preds = self.nn_model(batch_graph)
                        else:
                            features = torch.stack([featurize_program(leaf.program) 
                                                   for leaf, _ in uncached_leaves]).to(self.device)
                            _, value_preds = self.nn_model(features)
                        
                        # åˆ†é…rewardså¹¶æ›´æ–°ç¼“å­˜
                        for idx, (leaf, path) in enumerate(uncached_leaves):
                            value_est = value_preds[idx].item() if value_preds.dim() > 0 else value_preds.item()
                            reward = value_est * 10.0
                            
                            # æ›´æ–°ç¼“å­˜
                            prog_hash = get_program_hash(leaf.program)
                            gnn_prior_cache[prog_hash] = (1.0 / len(EDIT_TYPES), value_est)
                            
                            for node in reversed(path):
                                node.visits += 1
                                node.value_sum += reward
                except Exception:
                    # æ‰¹é‡æ¨ç†å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤å€¼
                    for leaf, path in uncached_leaves:
                        reward = -10.0
                        for node in reversed(path):
                            node.visits += 1
                            node.value_sum += reward
        
        # ğŸ“Š æ€§èƒ½ç»Ÿè®¡ (å¯é€‰ï¼Œç”¨äºè°ƒè¯•)
        if hasattr(self, '_mcts_stats'):
            total_gnn_calls = len(pending_gnn_nodes) + len(uncached_leaves if 'uncached_leaves' in locals() else [])
            cached_hits = len(cached_leaves if 'cached_leaves' in locals() else [])
            self._mcts_stats['total_gnn_nodes'] = self._mcts_stats.get('total_gnn_nodes', 0) + len(pending_gnn_nodes)
            self._mcts_stats['total_value_cached'] = self._mcts_stats.get('total_value_cached', 0) + cached_hits
            self._mcts_stats['cache_size'] = len(gnn_prior_cache)
        
        # è¿”å›rootçš„å­èŠ‚ç‚¹å’Œè®¿é—®åˆ†å¸ƒ
        if root.children:
            visit_counts = [child.visits for child in root.children]
            return root.children, visit_counts
        else:
            return [], []
    
    def _select_child_puct(self, node: MCTSNode) -> MCTSNode:
        """PUCTé€‰æ‹©ï¼ˆä½¿ç”¨NNå…ˆéªŒï¼‰"""
        if not node.children:
            return node
        
        best_score = -float('inf')
        best_child = None
        
        sqrt_n = np.sqrt(node.visits)
        c_puct = self.args.puct_c
        
        for child in node.children:
            # Qå€¼ï¼šå¹³å‡å¥–åŠ±
            q = child.value_sum / child.visits if child.visits > 0 else 0.0
            
            # Uå€¼ï¼šæ¢ç´¢å¥–åŠ±ï¼ˆä½¿ç”¨NNå…ˆéªŒï¼‰
            prior = getattr(child, '_prior_p', 1.0 / len(node.children))
            u = c_puct * prior * sqrt_n / (1 + child.visits)
            
            score = q + u
            
            if score > best_score:
                best_score = score
                best_child = child
        
        return best_child if best_child else node.children[0]
    
    def train_step(self):
        """å•æ­¥è®­ç»ƒ"""
        if len(self.replay_buffer) < self.args.batch_size:
            return
        
        # é‡‡æ ·batch
        batch = self.replay_buffer.sample(self.args.batch_size)
        
        # æ„å»ºtensorï¼ˆæ ¹æ®æ¨¡å¼ï¼‰
        if self.use_gnn:
            # GNNæ¨¡å¼ï¼šä½¿ç”¨PyG Batch
            graph_list = [s['graph'] for s in batch]
            batch_graph = PyGBatch.from_data_list(graph_list).to(self.device)
            policy_targets = torch.stack([s['policy_target'] for s in batch]).to(self.device)
            value_targets = torch.stack([s['value_target'] for s in batch]).to(self.device)
            
            # å‰å‘ä¼ æ’­
            policy_logits, value_preds = self.nn_model(batch_graph)
        else:
            # å›ºå®šç‰¹å¾æ¨¡å¼
            features = torch.stack([s['features'] for s in batch]).to(self.device)
            policy_targets = torch.stack([s['policy_target'] for s in batch]).to(self.device)
            value_targets = torch.stack([s['value_target'] for s in batch]).to(self.device)
            
            # å‰å‘ä¼ æ’­
            policy_logits, value_preds = self.nn_model(features)
        
        # æŸå¤±è®¡ç®—
        # ç­–ç•¥æŸå¤±ï¼šäº¤å‰ç†µï¼ˆMCTSè®¿é—®åˆ†å¸ƒä½œä¸ºç›®æ ‡ï¼‰
        policy_loss = -(policy_targets * F.log_softmax(policy_logits, dim=-1)).sum(dim=-1).mean()
        
        # ä»·å€¼æŸå¤±ï¼šMSE
        value_loss = F.mse_loss(value_preds.squeeze(), value_targets.squeeze())
        
        # æ€»æŸå¤±
        total_loss = policy_loss + self.args.value_loss_weight * value_loss
        
        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.nn_model.parameters(), 1.0)
        self.optimizer.step()
        
        return {
            'policy_loss': policy_loss.item(),
            'value_loss': value_loss.item(),
            'total_loss': total_loss.item()
        }
    
    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        print(f"\n{'='*80}")
        print(f"å¼€å§‹åœ¨çº¿è®­ç»ƒ - AlphaZeroå¼ç¨‹åºåˆæˆ")
        print(f"{'='*80}")
        print(f"æ€»è¿­ä»£æ•°: {self.args.total_iters}")
        print(f"MCTSæ¨¡æ‹Ÿæ•°/è¿­ä»£: {self.args.mcts_simulations}")
        print(f"NNæ›´æ–°é¢‘ç‡: æ¯{self.args.update_freq}æ¬¡è¿­ä»£")
        print(f"æ‰¹é‡å¤§å°: {self.args.batch_size}")
        print(f"{'='*80}\n")
        
        # åˆå§‹åŒ–ç¨‹åºï¼ˆæ”¯æŒä»æ–‡ä»¶åŠ è½½ï¼‰
        if hasattr(self.args, 'warm_start') and self.args.warm_start:
            loaded_program = self._load_program_from_json(self.args.warm_start)
            if loaded_program:
                current_program = loaded_program
                print(f"[Trainer] ğŸ”¥ Warm Start: ä½¿ç”¨é¢„è®­ç»ƒç¨‹åº ({len(current_program)} æ¡è§„åˆ™)")
            else:
                current_program = self._generate_random_program()
                print(f"[Trainer] âš ï¸ Warm Start å¤±è´¥ï¼Œä½¿ç”¨éšæœºåˆå§‹åŒ–")
        else:
            current_program = self._generate_random_program()
        
        for iter_idx in range(self.args.total_iters):
            iter_start_time = time.time()
            
            print(f"\n[Iter {iter_idx+1}/{self.args.total_iters}] MCTSæœç´¢ä¸­...")
            
            # MCTSæœç´¢
            children, visit_counts = self.mcts_search(current_program, self.args.mcts_simulations)
            
            if not children:
                print(f"[Iter {iter_idx+1}] âš ï¸ æœªç”Ÿæˆå­èŠ‚ç‚¹ï¼Œè·³è¿‡")
                continue
            
            # é€‰æ‹©è®¿é—®æœ€å¤šçš„å­èŠ‚ç‚¹
            best_child_idx = np.argmax(visit_counts)
            best_child = children[best_child_idx]
            next_program = best_child.program
            
            # çœŸå®è¯„ä¼°ï¼ˆæ¯æ¬¡è¿­ä»£è‡³å°‘1æ¬¡ï¼‰
            reward = self.evaluator.evaluate_single(next_program)
            
            # æ”¶é›†è®­ç»ƒæ ·æœ¬
            # ç­–ç•¥æ ‡ç­¾ï¼šå°†æ ¹å­èŠ‚ç‚¹è®¿é—®åˆ†å¸ƒæŒ‰å…¶ç¼–è¾‘ç±»å‹èšåˆåˆ° EDIT_TYPES
            total_visits = sum(visit_counts)
            policy_target = torch.zeros(len(EDIT_TYPES))
            if total_visits > 0:
                for i, child in enumerate(children):
                    prob = float(visit_counts[i]) / float(total_visits)
                    et = getattr(child, '_edit_type', None)
                    if et in EDIT_TYPES:
                        policy_target[EDIT_TYPES.index(et)] += prob
                    else:
                        # è‹¥æœªçŸ¥ç±»å‹ï¼Œç­‰é‡åˆ†æ‘Šåˆ°æ‰€æœ‰ç»´åº¦ï¼Œé¿å…ä¸¢å¤±æ¦‚ç‡è´¨é‡
                        policy_target += prob / len(EDIT_TYPES)
                # å½’ä¸€åŒ–ï¼ˆæ•°å€¼å®‰å…¨ï¼‰
                s = float(policy_target.sum().item())
                if s > 0:
                    policy_target = policy_target / s
            else:
                # æ²¡æœ‰è®¿é—®è®¡æ•°æ—¶ï¼Œé€€åŒ–ä¸ºå‡åŒ€åˆ†å¸ƒ
                policy_target += 1.0 / len(EDIT_TYPES)
            
            # ä»·å€¼æ ‡ç­¾ï¼šå½’ä¸€åŒ–å¥–åŠ±
            value_target = torch.tensor([reward / 10.0], dtype=torch.float32)  # ç¼©æ”¾åˆ° [-1, 1]
            
            # æ„å»ºæ ·æœ¬ï¼ˆæ ¹æ®æ¨¡å¼é€‰æ‹©ç‰¹å¾æˆ–å›¾ï¼‰
            if self.use_gnn:
                sample = {
                    'graph': ast_to_pyg_graph(current_program),
                    'policy_target': policy_target,
                    'value_target': value_target
                }
            else:
                sample = {
                    'features': featurize_program(current_program),
                    'policy_target': policy_target,
                    'value_target': value_target
                }
            
            self.replay_buffer.push(sample)
            
            # æ›´æ–°NNï¼ˆæ¯Næ¬¡è¿­ä»£ï¼‰
            if (iter_idx + 1) % self.args.update_freq == 0:
                print(f"[Iter {iter_idx+1}] æ›´æ–°NN...")
                for _ in range(self.args.train_steps_per_update):
                    losses = self.train_step()
                    if losses:
                        print(f"  Loss: policy={losses['policy_loss']:.4f}, value={losses['value_loss']:.4f}")
            
            # æ›´æ–°æœ€ä½³ç¨‹åº
            if reward > self.best_reward:
                self.best_reward = reward
                self.best_program = next_program
                print(f"[Iter {iter_idx+1}] ğŸ‰ æ–°æœ€ä½³ï¼å¥–åŠ±: {reward:.4f}")
                
                # ä¿å­˜
                save_program_json(self.best_program, self.args.save_path)
            
            # æ›´æ–°å½“å‰ç¨‹åº
            current_program = next_program
            
            iter_time = time.time() - iter_start_time
            
            # ğŸ“Š MCTSæ€§èƒ½ç»Ÿè®¡ (æ¯10è½®è¾“å‡ºä¸€æ¬¡)
            mcts_info = ""
            if self._mcts_stats and (iter_idx + 1) % 10 == 0:
                total_gnn = self._mcts_stats.get('total_gnn_nodes', 0)
                total_cached = self._mcts_stats.get('total_value_cached', 0)
                cache_size = self._mcts_stats.get('cache_size', 0)
                if total_gnn > 0:
                    hit_rate = total_cached / (total_gnn + total_cached) * 100 if (total_gnn + total_cached) > 0 else 0
                    mcts_info = f" | GNN: {total_gnn}èŠ‚ç‚¹ | ç¼“å­˜å‘½ä¸­: {hit_rate:.0f}% ({cache_size}é¡¹)"
                # é‡ç½®ç»Ÿè®¡
                self._mcts_stats = {}
            
            print(f"[Iter {iter_idx+1}] å®Œæˆ | å¥–åŠ±: {reward:.4f} | è€—æ—¶: {iter_time:.1f}s | Buffer: {len(self.replay_buffer)}{mcts_info}")
            
            # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
            if (iter_idx + 1) % self.args.checkpoint_freq == 0:
                checkpoint_path = f"{self.args.save_path.replace('.json', '')}_nn_iter_{iter_idx+1}.pt"
                torch.save(self.nn_model.state_dict(), checkpoint_path)
                print(f"[Iter {iter_idx+1}] ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")
        
        print(f"\n{'='*80}")
        print(f"è®­ç»ƒå®Œæˆï¼æœ€ä½³å¥–åŠ±: {self.best_reward:.4f}")
        print(f"{'='*80}\n")


def parse_args():
    p = argparse.ArgumentParser(description='åœ¨çº¿è®­ç»ƒ - AlphaZeroå¼ç¨‹åºåˆæˆ')
    
    # è®­ç»ƒå‚æ•°
    p.add_argument('--total-iters', type=int, default=5000, help='æ€»è¿­ä»£æ•°')
    p.add_argument('--mcts-simulations', type=int, default=800, help='æ¯æ¬¡è¿­ä»£çš„MCTSæ¨¡æ‹Ÿæ•°')
    p.add_argument('--update-freq', type=int, default=50, help='NNæ›´æ–°é¢‘ç‡')
    p.add_argument('--train-steps-per-update', type=int, default=10, help='æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒæ­¥æ•°')
    p.add_argument('--batch-size', type=int, default=256, help='æ‰¹é‡å¤§å°')
    p.add_argument('--replay-capacity', type=int, default=50000, help='ç»éªŒå›æ”¾å®¹é‡')
    
    # NNå‚æ•°
    p.add_argument('--use-gnn', action='store_true', help='ä½¿ç”¨GNNç½‘ç»œï¼ˆGATï¼‰ä»£æ›¿å›ºå®šç‰¹å¾ç½‘ç»œ')
    p.add_argument('--nn-version', type=str, default='v1', choices=['v1','v2'], help='GNNç‰ˆæœ¬: v1(åŸå§‹) æˆ– v2(åˆ†å±‚åŒç½‘ç»œ)')
    p.add_argument('--nn-hidden', type=int, default=256, help='NNéšè—å±‚ç»´åº¦')
    p.add_argument('--learning-rate', type=float, default=1e-3, help='å­¦ä¹ ç‡')
    p.add_argument('--value-loss-weight', type=float, default=0.5, help='ä»·å€¼æŸå¤±æƒé‡')
    
    # MCTSå‚æ•°
    p.add_argument('--exploration-weight', type=float, default=1.4, help='UCBæ¢ç´¢æƒé‡')
    p.add_argument('--puct-c', type=float, default=1.5, help='PUCTå¸¸æ•°')
    p.add_argument('--max-depth', type=int, default=20, help='MCTSæœ€å¤§æ·±åº¦')
    p.add_argument('--real-sim-frac', type=float, default=0.8, help='MCTSæ¨¡æ‹Ÿä¸­ä½¿ç”¨çœŸå®ä»¿çœŸçš„æ¯”ä¾‹ [0,1]ï¼Œé»˜è®¤0.8ä¿è¯æ•°æ®è´¨é‡')
    
    # ä»¿çœŸå‚æ•°ï¼ˆä»…Isaac Gymï¼‰
    p.add_argument('--traj', type=str, default='figure8', choices=['hover', 'figure8', 'circle', 'helix'])
    p.add_argument('--duration', type=int, default=10, help='ä»¿çœŸæ—¶é•¿ï¼ˆç§’ï¼‰')
    p.add_argument('--isaac-num-envs', type=int, default=512, help='Isaac Gymå¹¶è¡Œç¯å¢ƒæ•°')
    p.add_argument('--eval-replicas-per-program', type=int, default=1, help='evaluate_single æ—¶å¹¶è¡Œå‰¯æœ¬æ•°ï¼Œå–å¹³å‡ä»¥æé«˜åˆ©ç”¨ç‡/ç¨³å®šæ€§')
    p.add_argument('--min-steps-frac', type=float, default=0.0, help='æ¯æ¬¡è¯„ä¼°è‡³å°‘æ‰§è¡Œçš„æ­¥æ•°æ¯”ä¾‹ [0,1]ï¼Œé¿å…è¿‡æ—© done é€€å‡º')
    p.add_argument('--reward-reduction', type=str, default='sum', choices=['sum','mean'], help="å¥–åŠ±å½’çº¦æ–¹å¼ï¼š'sum'ï¼ˆæ­¥æ¬¡æ±‚å’Œï¼‰æˆ– 'mean'ï¼ˆæ­¥æ¬¡å¹³å‡ï¼‰")
    p.add_argument('--use-fast-path', action='store_true', help='å¯ç”¨è¶…é«˜æ€§èƒ½ä¼˜åŒ–è·¯å¾„ï¼ˆç¯å¢ƒæ± å¤ç”¨+Numba JITç¼–è¯‘ï¼Œ7Ã—åŠ é€Ÿï¼‰')
    p.add_argument('--use-dummy-eval', action='store_true', help='å¼ºåˆ¶ä½¿ç”¨Dummyè¯„ä¼°å™¨ï¼ˆç¦ç”¨Isaac Gymï¼‰ï¼Œç”¨äºå¿«é€ŸA/BåŸºå‡†')
    
    # ä¿å­˜å‚æ•°
    p.add_argument('--save-path', type=str, default='01_pi_flight/results/online_best_program.json')
    p.add_argument('--checkpoint-freq', type=int, default=50, help='æ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡ï¼ˆé»˜è®¤50ï¼‰')
    p.add_argument('--warm-start', type=str, default=None, help='ä»å·²æœ‰ç¨‹åºæ–‡ä»¶å¼€å§‹è®­ç»ƒï¼ˆJSON è·¯å¾„ï¼‰')
    
    return p.parse_args()


if __name__ == '__main__':
    args = parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    random.seed(42)
    
    # å¼€å§‹è®­ç»ƒ
    trainer = OnlineTrainer(args)
    trainer.train()
