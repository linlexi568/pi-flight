#!/usr/bin/env python3
"""
Population-Based Training (PBT) for MCTS-based Program Synthesis
=================================================================

16-agentå¹¶è¡Œè®­ç»ƒï¼Œè‡ªåŠ¨è°ƒèŠ‚MCTSè¶…å‚æ•°ï¼š
- æ¯ä¸ªagentç‹¬ç«‹è®­ç»ƒGNN+MCTS
- å‘¨æœŸæ€§è¯„ä¼°æ€§èƒ½ï¼Œæ·˜æ±°å¼±agent
- å¤åˆ¶å¼ºagentçš„æƒé‡+æ‰°åŠ¨å‚æ•°
- å…±äº«Isaac Gymç¯å¢ƒæ± ï¼ˆ512ç¯å¢ƒï¼‰

å‚è€ƒæ–‡çŒ®:
- PBT: Jaderberg et al. (2018) "Population Based Training of Neural Networks"
- AlphaZero: Silver et al. (2017) "Mastering Chess and Shogi by Self-Play"
"""

import argparse
import time
import json
import random
import copy
import os
from typing import List, Dict, Any, Tuple, Optional
from collections import deque
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# å¯¼å…¥åŸºç¡€è®­ç»ƒæ¨¡å—
from train_online import (
    OnlineTrainer, ReplayBuffer, 
    EDIT_TYPES, ast_to_pyg_graph,
    save_program_json, get_program_hash
)

# GNNæ¨¡å—
from models.gnn_policy_nn_v2 import create_gnn_policy_value_net_v2
from torch_geometric.data import Batch as PyGBatch

# Evaluator
from utils.batch_evaluation import BatchEvaluator


class PBTAgent:
    """å•ä¸ªPBT agentï¼ˆåŒ…å«GNNæ¨¡å‹ã€MCTSå‚æ•°ã€è®­ç»ƒçŠ¶æ€ï¼‰"""
    
    def __init__(self, agent_id: int, args, device, shared_encoder=None):
        self.id = agent_id
        self.args = args
        self.device = device
        
        # MCTSå‚æ•°ï¼ˆå¯æ¼”åŒ–çš„ï¼‰
        self.mcts_params = self._initialize_mcts_params()
        
        # è®­ç»ƒè¶…å‚ï¼ˆå¯æ¼”åŒ–çš„ï¼‰
        self.learning_rate = 10 ** np.random.uniform(-4, -2.5)  # 1e-4 åˆ° 3e-3
        
        # GNNæ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨å…±äº«ç¼–ç å™¨ï¼‰
        self.shared_encoder = shared_encoder
        if shared_encoder is not None:
            # åªåˆ›å»ºç‹¬ç«‹çš„policy head
            self.policy_head = self._create_policy_head()
            self.nn_model = None  # æ ‡è®°ä½¿ç”¨å…±äº«æ¨¡å¼
        else:
            # åˆ›å»ºå®Œæ•´çš„ç‹¬ç«‹æ¨¡å‹
            self.nn_model = create_gnn_policy_value_net_v2(
                node_feat_dim=args.node_feat_dim if hasattr(args, 'node_feat_dim') else 16,
                hidden_channels=args.hidden_channels if hasattr(args, 'hidden_channels') else 128,
                num_gnn_layers=args.num_gnn_layers if hasattr(args, 'num_gnn_layers') else 3,
                n_edit_types=len(EDIT_TYPES),
                dropout=0.1
            ).to(device)
            self.policy_head = None
        
        # ä¼˜åŒ–å™¨
        self._setup_optimizer()
        
        # Replay buffer
        self.replay_buffer = ReplayBuffer(capacity=args.replay_capacity)
        
        # æ€§èƒ½è¿½è¸ª
        self.performance_history = deque(maxlen=20)  # æœ€è¿‘20è½®
        self.best_reward = -float('inf')
        self.best_program = None
        self.total_iterations = 0
        
        # ä»OnlineTrainerç»§æ‰¿çš„è®­ç»ƒå™¨ï¼ˆå¤ç”¨MCTSé€»è¾‘ï¼‰
        self.trainer = None  # ç¨ååˆå§‹åŒ–
        
        print(f"[Agent {self.id}] åˆå§‹åŒ–å®Œæˆï¼ŒMCTSå‚æ•°: {self.mcts_params}")
    
    def _initialize_mcts_params(self) -> Dict[str, float]:
        """éšæœºåˆå§‹åŒ–MCTSå‚æ•°"""
        return {
            'puct_c': np.random.uniform(1.0, 2.5),
            'exploration_weight': np.random.uniform(1.5, 4.0),
            'dirichlet_eps': np.random.uniform(0.15, 0.45),
            'dirichlet_alpha': np.random.uniform(0.2, 0.5),
            'temperature': np.random.uniform(0.6, 1.8),
            'simulations': int(np.random.choice([400, 600, 800]))  # é™ä½ä»¥æ”¯æŒ16 agents
        }
    
    def _create_policy_head(self) -> nn.Module:
        """åˆ›å»ºç‹¬ç«‹çš„policy headï¼ˆç”¨äºå…±äº«ç¼–ç å™¨æ¨¡å¼ï¼‰"""
        hidden_dim = self.args.hidden_channels if hasattr(self.args, 'hidden_channels') else 128
        
        head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, len(EDIT_TYPES))
        ).to(self.device)
        
        return head
    
    def _setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        if self.shared_encoder is not None:
            # å…±äº«æ¨¡å¼ï¼šä¼˜åŒ–shared_encoder + policy_head
            params = list(self.shared_encoder.parameters()) + list(self.policy_head.parameters())
        else:
            # ç‹¬ç«‹æ¨¡å¼ï¼šä¼˜åŒ–æ•´ä¸ªnn_model
            params = self.nn_model.parameters()
        
        self.optimizer = optim.Adam(params, lr=self.learning_rate)
    
    def forward(self, graph):
        """å‰å‘ä¼ æ’­"""
        if self.shared_encoder is not None:
            # å…±äº«ç¼–ç å™¨æ¨¡å¼
            embedding = self.shared_encoder.get_embedding(graph)
            policy_logits = self.policy_head(embedding)
            return policy_logits, None, None  # è¿”å›æ ¼å¼å…¼å®¹
        else:
            # ç‹¬ç«‹æ¨¡å‹æ¨¡å¼
            return self.nn_model(graph)
    
    def get_model_state_dict(self):
        """è·å–æ¨¡å‹çŠ¶æ€å­—å…¸"""
        if self.shared_encoder is not None:
            return {
                'shared_encoder': self.shared_encoder.state_dict(),
                'policy_head': self.policy_head.state_dict()
            }
        else:
            return {'nn_model': self.nn_model.state_dict()}
    
    def load_model_state_dict(self, state_dict):
        """åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸"""
        if self.shared_encoder is not None:
            self.shared_encoder.load_state_dict(state_dict['shared_encoder'])
            self.policy_head.load_state_dict(state_dict['policy_head'])
        else:
            self.nn_model.load_state_dict(state_dict['nn_model'])
    
    def copy_from(self, other_agent: 'PBTAgent'):
        """ä»å¦ä¸€ä¸ªagentå¤åˆ¶æƒé‡"""
        self.load_model_state_dict(other_agent.get_model_state_dict())
        print(f"[Agent {self.id}] å¤åˆ¶ Agent {other_agent.id} çš„æƒé‡")
    
    def perturb_params(self, perturb_factors=(0.8, 1.2)):
        """æ‰°åŠ¨MCTSå‚æ•°"""
        for key, value in self.mcts_params.items():
            if key == 'simulations':
                # simulationsç¦»æ•£é€‰æ‹©
                continue
            factor = random.choice(perturb_factors)
            new_value = value * factor
            
            # çº¦æŸåˆ°åˆç†èŒƒå›´
            if key == 'puct_c':
                new_value = np.clip(new_value, 0.5, 3.0)
            elif key == 'exploration_weight':
                new_value = np.clip(new_value, 1.0, 5.0)
            elif key == 'dirichlet_eps':
                new_value = np.clip(new_value, 0.05, 0.6)
            elif key == 'dirichlet_alpha':
                new_value = np.clip(new_value, 0.1, 0.7)
            elif key == 'temperature':
                new_value = np.clip(new_value, 0.3, 2.5)
            
            self.mcts_params[key] = new_value
        
        # å­¦ä¹ ç‡ä¹Ÿæ‰°åŠ¨
        self.learning_rate *= random.choice(perturb_factors)
        self.learning_rate = np.clip(self.learning_rate, 1e-5, 1e-2)
        
        # æ›´æ–°ä¼˜åŒ–å™¨å­¦ä¹ ç‡
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.learning_rate
        
        print(f"[Agent {self.id}] å‚æ•°æ‰°åŠ¨å®Œæˆ: {self.mcts_params}, lr={self.learning_rate:.2e}")


class PBTTrainer:
    """Population-Based Training ä¸»è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"\n{'='*60}")
        print(f"Population-Based Training for MCTS Program Synthesis")
        print(f"{'='*60}")
        print(f"Agentæ•°é‡: {args.n_agents}")
        print(f"PBTé—´éš”: æ¯{args.pbt_interval}è½®è¯„ä¼°ä¸€æ¬¡")
        print(f"æ·˜æ±°ç‡: {args.exploit_threshold*100:.0f}%")
        print(f"è®¾å¤‡: {self.device}")
        print(f"{'='*60}\n")
        
        # åˆå§‹åŒ–evaluatorï¼ˆæ‰€æœ‰agentå…±äº«ï¼‰
        self.evaluator = BatchEvaluator(
            trajectory_type=args.traj,
            duration=args.duration,
            num_envs=args.isaac_num_envs,
            device='cuda:0',
            headless=True,
            reward_profile=args.reward_profile
        )
        
        # åˆ›å»ºå…±äº«çš„GNNç¼–ç å™¨ï¼ˆå¯é€‰ï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
        if args.shared_encoder:
            print("[PBT] ä½¿ç”¨å…±äº«GNNç¼–ç å™¨æ¨¡å¼ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
            self.shared_encoder = self._create_shared_encoder()
        else:
            print("[PBT] ä½¿ç”¨ç‹¬ç«‹GNNæ¨¡å‹æ¨¡å¼")
            self.shared_encoder = None
        
        # åˆå§‹åŒ–æ‰€æœ‰agents
        self.agents: List[PBTAgent] = []
        for i in range(args.n_agents):
            agent = PBTAgent(i, args, self.device, self.shared_encoder)
            self.agents.append(agent)
        
        # PBTç»Ÿè®¡
        self.global_best_reward = -float('inf')
        self.global_best_program = None
        self.global_best_agent_id = -1
        
        # æ¯ä¸ªagentçš„è®­ç»ƒå™¨ï¼ˆå¤ç”¨OnlineTrainerçš„MCTSé€»è¾‘ï¼‰
        self._setup_agent_trainers()
    
    def _create_shared_encoder(self):
        """åˆ›å»ºå…±äº«çš„GNNç¼–ç å™¨"""
        model = create_gnn_policy_value_net_v2(
            node_feat_dim=self.args.node_feat_dim if hasattr(self.args, 'node_feat_dim') else 16,
            hidden_channels=self.args.hidden_channels if hasattr(self.args, 'hidden_channels') else 128,
            num_gnn_layers=self.args.num_gnn_layers if hasattr(self.args, 'num_gnn_layers') else 3,
            n_edit_types=len(EDIT_TYPES),
            dropout=0.1
        ).to(self.device)
        
        return model.gnn_encoder  # åªè¿”å›ç¼–ç å™¨éƒ¨åˆ†
    
    def _setup_agent_trainers(self):
        """ä¸ºæ¯ä¸ªagentåˆ›å»ºè®­ç»ƒå™¨å®ä¾‹ï¼ˆå¤ç”¨OnlineTrainerçš„MCTSé€»è¾‘ï¼‰"""
        for agent in self.agents:
            # åˆ›å»ºä¸€ä¸ªè½»é‡çº§çš„trainer wrapper
            # è¿™é‡Œæˆ‘ä»¬å¤ç”¨OnlineTrainerçš„mcts_searchæ–¹æ³•
            # ä½†ä½¿ç”¨agentè‡ªå·±çš„å‚æ•°
            agent.trainer = self._create_agent_trainer_wrapper(agent)
    
    def _create_agent_trainer_wrapper(self, agent: PBTAgent):
        """åˆ›å»ºagentçš„è®­ç»ƒå™¨wrapper"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„wrapperï¼Œå¤ç”¨OnlineTrainerçš„æ ¸å¿ƒæ–¹æ³•
        class AgentTrainerWrapper:
            def __init__(self, parent_trainer, agent):
                self.parent = parent_trainer
                self.agent = agent
                self.args = agent.args
                self.device = agent.device
                self.evaluator = parent_trainer.evaluator
                
                # ä½¿ç”¨agentçš„æ¨¡å‹
                if agent.nn_model is not None:
                    self.nn_model = agent.nn_model
                else:
                    # å…±äº«ç¼–ç å™¨æ¨¡å¼ï¼šåˆ›å»ºä¸´æ—¶çš„forward wrapper
                    self.nn_model = lambda graph: agent.forward(graph)
                
                # ä½¿ç”¨agentçš„MCTSå‚æ•°
                self._update_mcts_params()
            
            def _update_mcts_params(self):
                """ä»agentåŒæ­¥MCTSå‚æ•°"""
                params = self.agent.mcts_params
                self._puct_c = params['puct_c']
                self._exploration_weight = params['exploration_weight']
                self._root_dirichlet_eps = params['dirichlet_eps']
                self._root_dirichlet_alpha = params['dirichlet_alpha']
                self._policy_temperature = params['temperature']
                self._max_depth = 12  # å›ºå®š
            
            def mcts_search(self, *args, **kwargs):
                """è°ƒç”¨çˆ¶trainerçš„mcts_searchï¼ˆä½†ä½¿ç”¨agentçš„å‚æ•°ï¼‰"""
                # è¿™é‡Œéœ€è¦å¤ç”¨OnlineTrainer.mcts_searchçš„å®Œæ•´é€»è¾‘
                # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æš‚æ—¶æ ‡è®°ä¸ºTODO
                # å®é™…å®ç°ä¸­éœ€è¦å®Œæ•´å¤åˆ¶æˆ–é‡æ„mcts_search
                pass  # TODO: å®ç°
        
        return AgentTrainerWrapper(self, agent)
    
    def train(self):
        """PBTä¸»è®­ç»ƒå¾ªç¯"""
        for global_iter in range(self.args.total_iters):
            iter_start = time.time()
            
            print(f"\n{'='*60}")
            print(f"Iteration {global_iter + 1} / {self.args.total_iters}")
            print(f"{'='*60}")
            
            # 1. æ‰€æœ‰agentå¹¶è¡Œè®­ç»ƒä¸€æ­¥
            agent_rewards = self._parallel_train_step(global_iter)
            
            # 2. æ›´æ–°å…¨å±€æœ€ä½³
            self._update_global_best(agent_rewards, global_iter)
            
            # 3. PBTè°ƒåº¦ï¼ˆå‘¨æœŸæ€§ï¼‰
            if (global_iter + 1) % self.args.pbt_interval == 0:
                self._pbt_exploit_explore()
            
            # 4. ç»Ÿè®¡è¾“å‡º
            iter_time = time.time() - iter_start
            self._print_statistics(global_iter, agent_rewards, iter_time)
            
            # 5. ä¿å­˜checkpoint
            if (global_iter + 1) % self.args.save_freq == 0:
                self._save_checkpoint(global_iter)
        
        # æœ€ç»ˆä¿å­˜
        self._save_final_results()
    
    def _parallel_train_step(self, iteration: int) -> List[float]:
        """æ‰€æœ‰agentå¹¶è¡Œè®­ç»ƒä¸€æ­¥ï¼Œè¿”å›æ¯ä¸ªagentçš„å¥–åŠ±"""
        agent_rewards = []
        
        # TODO: è¿™é‡Œéœ€è¦å®ç°æ¯ä¸ªagentçš„å®Œæ•´è®­ç»ƒæ­¥éª¤
        # åŒ…æ‹¬ï¼šMCTSæœç´¢ -> ç”Ÿæˆç¨‹åº -> è¯„ä¼° -> æ›´æ–°NN
        
        # ç®€åŒ–ç¤ºä¾‹ï¼ˆéœ€è¦å®Œæ•´å®ç°ï¼‰
        for agent in self.agents:
            # 1. è¿è¡ŒMCTSï¼ˆä½¿ç”¨agentçš„å‚æ•°ï¼‰
            # next_program = agent.trainer.mcts_search(...)
            
            # 2. è¯„ä¼°ç¨‹åº
            # reward = self.evaluator.evaluate_single(next_program)
            
            # 3. æ·»åŠ åˆ°replay buffer
            # agent.replay_buffer.push(sample)
            
            # 4. æ›´æ–°NNï¼ˆæ¯Nè½®ï¼‰
            # if iteration % agent.args.update_freq == 0:
            #     agent_train_step(agent)
            
            # ä¸´æ—¶ï¼šéšæœºå¥–åŠ±ï¼ˆæµ‹è¯•PBTé€»è¾‘ï¼‰
            reward = np.random.uniform(-10, 0)
            agent.performance_history.append(reward)
            if reward > agent.best_reward:
                agent.best_reward = reward
            
            agent_rewards.append(reward)
        
        return agent_rewards
    
    def _update_global_best(self, agent_rewards: List[float], iteration: int):
        """æ›´æ–°å…¨å±€æœ€ä½³agent"""
        for i, (agent, reward) in enumerate(zip(self.agents, agent_rewards)):
            if reward > self.global_best_reward:
                self.global_best_reward = reward
                self.global_best_program = agent.best_program
                self.global_best_agent_id = agent.id
                print(f"[Iter {iteration+1}] ğŸ‰ æ–°å…¨å±€æœ€ä½³ï¼Agent {agent.id}, Reward: {reward:.4f}")
    
    def _pbt_exploit_explore(self):
        """PBTçš„æ ¸å¿ƒï¼šExploit & Explore"""
        print(f"\n{'='*60}")
        print(f"PBTè°ƒåº¦ï¼šExploit & Explore")
        print(f"{'='*60}")
        
        # 1. è®¡ç®—æ¯ä¸ªagentçš„æ€§èƒ½ï¼ˆæœ€è¿‘10è½®å¹³å‡ï¼‰
        performances = []
        for agent in self.agents:
            if len(agent.performance_history) > 0:
                recent = list(agent.performance_history)[-10:]
                perf = np.mean(recent)
            else:
                perf = -float('inf')
            performances.append((agent.id, perf, agent))
        
        # 2. æ’åº
        performances.sort(key=lambda x: x[1], reverse=True)
        
        # æ‰“å°æ’å
        print("\nAgentæ€§èƒ½æ’å:")
        for rank, (agent_id, perf, agent) in enumerate(performances, 1):
            marker = "ğŸ†" if rank <= 3 else ("â­" if rank <= len(self.agents)//2 else "")
            print(f"  {rank}. Agent {agent_id}: {perf:.4f} {marker}")
        
        # 3. æ·˜æ±°ä¸‹ä½20%ï¼Œå¤åˆ¶ä¸Šä½agent
        n_exploit = max(1, int(self.args.n_agents * self.args.exploit_threshold))
        
        top_agents = [agent for _, _, agent in performances[:n_exploit]]
        bottom_agents = [agent for _, _, agent in performances[-n_exploit:]]
        
        print(f"\næ·˜æ±°ä¸‹ä½{n_exploit}ä¸ªagentï¼Œå¤åˆ¶ä¸Šä½agent:")
        for weak_agent in bottom_agents:
            # éšæœºé€‰ä¸€ä¸ªå¼ºagent
            strong_agent = random.choice(top_agents)
            
            print(f"  ğŸ”„ Agent {weak_agent.id} (â­{weak_agent.best_reward:.2f}) "
                  f"å¤åˆ¶ Agent {strong_agent.id} (â­{strong_agent.best_reward:.2f})")
            
            # å¤åˆ¶æƒé‡
            weak_agent.copy_from(strong_agent)
            
            # æ‰°åŠ¨å‚æ•°
            weak_agent.perturb_params(perturb_factors=(0.8, 1.2))
        
        print(f"{'='*60}\n")
    
    def _print_statistics(self, iteration: int, agent_rewards: List[float], iter_time: float):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        mean_reward = np.mean(agent_rewards)
        max_reward = np.max(agent_rewards)
        min_reward = np.min(agent_rewards)
        std_reward = np.std(agent_rewards)
        
        print(f"\nç»Ÿè®¡:")
        print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.4f}")
        print(f"  æœ€å¤§å¥–åŠ±: {max_reward:.4f}")
        print(f"  æœ€å°å¥–åŠ±: {min_reward:.4f}")
        print(f"  æ ‡å‡†å·®: {std_reward:.4f}")
        print(f"  å…¨å±€æœ€ä½³: {self.global_best_reward:.4f} (Agent {self.global_best_agent_id})")
        print(f"  ç”¨æ—¶: {iter_time:.2f}s")
    
    def _save_checkpoint(self, iteration: int):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'iteration': iteration,
            'global_best_reward': self.global_best_reward,
            'global_best_agent_id': self.global_best_agent_id,
            'agents': []
        }
        
        for agent in self.agents:
            agent_data = {
                'id': agent.id,
                'mcts_params': agent.mcts_params,
                'learning_rate': agent.learning_rate,
                'best_reward': agent.best_reward,
                'model_state': agent.get_model_state_dict()
            }
            checkpoint['agents'].append(agent_data)
        
        save_path = self.args.save_path.replace('.json', f'_pbt_iter{iteration+1}.pt')
        torch.save(checkpoint, save_path)
        print(f"  ğŸ’¾ Checkpointå·²ä¿å­˜: {save_path}")
    
    def _save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒå®Œæˆï¼")
        print(f"å…¨å±€æœ€ä½³å¥–åŠ±: {self.global_best_reward:.4f}")
        print(f"æ¥è‡ªAgent {self.global_best_agent_id}")
        print(f"{'='*60}\n")
        
        # ä¿å­˜æœ€ä½³ç¨‹åº
        if self.global_best_program is not None:
            final_path = self.args.save_path.replace('.json', '_pbt_final.json')
            save_program_json(self.global_best_program, final_path)
            print(f"æœ€ä½³ç¨‹åºå·²ä¿å­˜: {final_path}")


def parse_args():
    p = argparse.ArgumentParser(description='Population-Based Training for MCTS Program Synthesis')
    
    # PBTå‚æ•°
    p.add_argument('--n-agents', type=int, default=16, help='Agentæ•°é‡')
    p.add_argument('--pbt-interval', type=int, default=50, help='PBTè°ƒåº¦é—´éš”ï¼ˆè½®æ•°ï¼‰')
    p.add_argument('--exploit-threshold', type=float, default=0.25, help='æ·˜æ±°æ¯”ä¾‹ï¼ˆ0-1ï¼‰')
    p.add_argument('--shared-encoder', action='store_true', help='ä½¿ç”¨å…±äº«GNNç¼–ç å™¨ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰')
    
    # è®­ç»ƒå‚æ•°
    p.add_argument('--total-iters', type=int, default=5000, help='æ€»è¿­ä»£æ•°')
    p.add_argument('--update-freq', type=int, default=50, help='NNæ›´æ–°é¢‘ç‡')
    p.add_argument('--train-steps-per-update', type=int, default=10, help='æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒæ­¥æ•°')
    p.add_argument('--batch-size', type=int, default=64, help='æ‰¹é‡å¤§å°ï¼ˆé™ä½ä»¥é€‚åº”å¤šagentï¼‰')
    p.add_argument('--replay-capacity', type=int, default=20000, help='æ¯ä¸ªagentçš„replay bufferå®¹é‡')
    
    # GNNå‚æ•°
    p.add_argument('--node-feat-dim', type=int, default=16, help='èŠ‚ç‚¹ç‰¹å¾ç»´åº¦')
    p.add_argument('--hidden-channels', type=int, default=128, help='éšè—å±‚ç»´åº¦')
    p.add_argument('--num-gnn-layers', type=int, default=3, help='GNNå±‚æ•°')
    p.add_argument('--learning-rate', type=float, default=1e-3, help='åˆå§‹å­¦ä¹ ç‡ï¼ˆPBTä¼šè°ƒæ•´ï¼‰')
    
    # ä»¿çœŸå‚æ•°
    p.add_argument('--traj', type=str, default='figure8', choices=['hover', 'figure8', 'circle', 'helix'])
    p.add_argument('--duration', type=int, default=10, help='ä»¿çœŸæ—¶é•¿ï¼ˆç§’ï¼‰')
    p.add_argument('--isaac-num-envs', type=int, default=512, help='Isaac Gymå¹¶è¡Œç¯å¢ƒæ•°')
    p.add_argument('--reward-profile', type=str, default='control_law_discovery', 
                   choices=['default', 'control_law_discovery', 'smooth_control', 'balanced_smooth'])
    
    # ä¿å­˜å‚æ•°
    p.add_argument('--save-path', type=str, default='results/pbt_best_program.json')
    p.add_argument('--save-freq', type=int, default=200, help='Checkpointä¿å­˜é¢‘ç‡')
    
    return p.parse_args()


if __name__ == '__main__':
    args = parse_args()
    
    # åˆ›å»ºPBTè®­ç»ƒå™¨
    pbt_trainer = PBTTrainer(args)
    
    # å¼€å§‹è®­ç»ƒ
    pbt_trainer.train()
