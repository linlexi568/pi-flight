#!/usr/bin/env python3
"""
Population-Based Training (PBT) for MCTS-based Program Synthesis
=================================================================

16-agentå¹¶è¡Œè®­ç»ƒï¼Œè‡ªåŠ¨è°ƒèŠ‚MCTSè¶…å‚æ•°ï¼š
- æ¯ä¸ªagentç‹¬ç«‹è®­ç»ƒGNN+MCTS
- å‘¨æœŸæ€§è¯„ä¼°æ€§èƒ½ï¼Œæ·˜æ±°å¼±agent
- å¤åˆ¶å¼ºagentçš„æƒé‡+æ‰°åŠ¨å‚æ•°
- å…±äº«Isaac Gymç¯å¢ƒæ± ï¼ˆ512ç¯å¢ƒï¼‰

å‚è€ƒæ–‡çŒ®:
- PBT: Jaderberg et al. (2018) "Population Based Training of Neural Networks"
- AlphaZero: Silver et al. (2017) "Mastering Chess and Shogi by Self-Play"
"""

import argparse
import time
import json
import random
import copy
import os
from typing import List, Dict, Any, Tuple, Optional, TYPE_CHECKING
from collections import deque
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim

# å¯¼å…¥åŸºç¡€è®­ç»ƒæ¨¡å—
from train_online import (
    OnlineTrainer, ReplayBuffer, 
    EDIT_TYPES, ast_to_pyg_graph,
    save_program_json, get_program_hash
)

# GNNæ¨¡å—
from models.gnn_policy_nn_v2 import create_gnn_policy_value_net_v2
from torch_geometric.data import Batch as PyGBatch

# Evaluator
from utils.batch_evaluation import BatchEvaluator

# MCTSç›¸å…³ï¼ˆä»…ç”¨äºç±»å‹æç¤ºï¼‰
if TYPE_CHECKING:
    from mcts_training.mcts import MCTS_Agent, MCTSNode


class PBTAgent:
    """å•ä¸ªPBT agentï¼ˆåŒ…å«GNNæ¨¡å‹ã€MCTSå‚æ•°ã€è®­ç»ƒçŠ¶æ€ï¼‰"""
    
    def __init__(self, agent_id: int, args, device, shared_encoder=None):
        self.id = agent_id
        self.args = args
        self.device = device
        
        # MCTSå‚æ•°ï¼ˆå¯æ¼”åŒ–çš„ï¼‰
        self.mcts_params = self._initialize_mcts_params()
        
        # è®­ç»ƒè¶…å‚ï¼ˆå¯æ¼”åŒ–çš„ï¼‰
        self.learning_rate = 10 ** np.random.uniform(-4, -2.5)  # 1e-4 åˆ° 3e-3
        
        # GNNæ¨¡å‹ï¼ˆå¦‚æœä½¿ç”¨å…±äº«ç¼–ç å™¨ï¼‰
        self.shared_encoder = shared_encoder
        if shared_encoder is not None:
            # åªåˆ›å»ºç‹¬ç«‹çš„policy head
            self.policy_head = self._create_policy_head()
            self.nn_model = None  # æ ‡è®°ä½¿ç”¨å…±äº«æ¨¡å¼
        else:
            # åˆ›å»ºå®Œæ•´çš„ç‹¬ç«‹æ¨¡å‹
            self.nn_model = create_gnn_policy_value_net_v2(
                node_feat_dim=args.node_feat_dim if hasattr(args, 'node_feat_dim') else 16,
                hidden_channels=args.hidden_channels if hasattr(args, 'hidden_channels') else 128,
                num_gnn_layers=args.num_gnn_layers if hasattr(args, 'num_gnn_layers') else 3,
                n_edit_types=len(EDIT_TYPES),
                dropout=0.1
            ).to(device)
            self.policy_head = None
        
        # ä¼˜åŒ–å™¨
        self._setup_optimizer()
        
        # Replay buffer
        self.replay_buffer = ReplayBuffer(capacity=args.replay_capacity)
        
        # æ€§èƒ½è¿½è¸ª
        self.performance_history = deque(maxlen=20)  # æœ€è¿‘20è½®
        self.best_reward = -float('inf')
        self.best_program = None
        self.total_iterations = 0
        
        # ä»OnlineTrainerç»§æ‰¿çš„è®­ç»ƒå™¨ï¼ˆå¤ç”¨MCTSé€»è¾‘ï¼‰
        self.trainer = None  # ç¨ååˆå§‹åŒ–
        
        print(f"[Agent {self.id}] åˆå§‹åŒ–å®Œæˆï¼ŒMCTSå‚æ•°: {self.mcts_params}")
    
    def _initialize_mcts_params(self) -> Dict[str, float]:
        """éšæœºåˆå§‹åŒ–MCTSå‚æ•°"""
        return {
            'puct_c': np.random.uniform(1.0, 2.5),
            'exploration_weight': np.random.uniform(1.5, 4.0),
            'dirichlet_eps': np.random.uniform(0.15, 0.45),
            'dirichlet_alpha': np.random.uniform(0.2, 0.5),
            'temperature': np.random.uniform(0.6, 1.8),
            'simulations': int(np.random.choice([400, 600, 800]))  # é™ä½ä»¥æ”¯æŒ16 agents
        }
    
    def _create_policy_head(self) -> nn.Module:
        """åˆ›å»ºç‹¬ç«‹çš„policy headï¼ˆç”¨äºå…±äº«ç¼–ç å™¨æ¨¡å¼ï¼‰"""
        hidden_dim = self.args.hidden_channels if hasattr(self.args, 'hidden_channels') else 128
        
        head = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(hidden_dim, len(EDIT_TYPES))
        ).to(self.device)
        
        return head
    
    def _setup_optimizer(self):
        """è®¾ç½®ä¼˜åŒ–å™¨"""
        if self.shared_encoder is not None:
            # å…±äº«æ¨¡å¼ï¼šä¼˜åŒ–shared_encoder + policy_head
            params = list(self.shared_encoder.parameters()) + list(self.policy_head.parameters())
        else:
            # ç‹¬ç«‹æ¨¡å¼ï¼šä¼˜åŒ–æ•´ä¸ªnn_model
            params = self.nn_model.parameters()
        
        self.optimizer = optim.Adam(params, lr=self.learning_rate)
    
    def forward(self, graph):
        """å‰å‘ä¼ æ’­"""
        if self.shared_encoder is not None:
            # å…±äº«ç¼–ç å™¨æ¨¡å¼
            embedding = self.shared_encoder.get_embedding(graph)
            policy_logits = self.policy_head(embedding)
            return policy_logits, None, None  # è¿”å›æ ¼å¼å…¼å®¹
        else:
            # ç‹¬ç«‹æ¨¡å‹æ¨¡å¼
            return self.nn_model(graph)
    
    def get_model_state_dict(self):
        """è·å–æ¨¡å‹çŠ¶æ€å­—å…¸"""
        if self.shared_encoder is not None:
            return {
                'shared_encoder': self.shared_encoder.state_dict(),
                'policy_head': self.policy_head.state_dict()
            }
        else:
            return {'nn_model': self.nn_model.state_dict()}
    
    def load_model_state_dict(self, state_dict):
        """åŠ è½½æ¨¡å‹çŠ¶æ€å­—å…¸"""
        if self.shared_encoder is not None:
            self.shared_encoder.load_state_dict(state_dict['shared_encoder'])
            self.policy_head.load_state_dict(state_dict['policy_head'])
        else:
            self.nn_model.load_state_dict(state_dict['nn_model'])
    
    def copy_from(self, other_agent: 'PBTAgent'):
        """ä»å¦ä¸€ä¸ªagentå¤åˆ¶æƒé‡"""
        self.load_model_state_dict(other_agent.get_model_state_dict())
        print(f"[Agent {self.id}] å¤åˆ¶ Agent {other_agent.id} çš„æƒé‡")
    
    def perturb_params(self, perturb_factors=(0.8, 1.2)):
        """æ‰°åŠ¨MCTSå‚æ•°"""
        for key, value in self.mcts_params.items():
            if key == 'simulations':
                # simulationsç¦»æ•£é€‰æ‹©
                continue
            factor = random.choice(perturb_factors)
            new_value = value * factor
            
            # çº¦æŸåˆ°åˆç†èŒƒå›´
            if key == 'puct_c':
                new_value = np.clip(new_value, 0.5, 3.0)
            elif key == 'exploration_weight':
                new_value = np.clip(new_value, 1.0, 5.0)
            elif key == 'dirichlet_eps':
                new_value = np.clip(new_value, 0.05, 0.6)
            elif key == 'dirichlet_alpha':
                new_value = np.clip(new_value, 0.1, 0.7)
            elif key == 'temperature':
                new_value = np.clip(new_value, 0.3, 2.5)
            
            self.mcts_params[key] = new_value
        
        # å­¦ä¹ ç‡ä¹Ÿæ‰°åŠ¨
        self.learning_rate *= random.choice(perturb_factors)
        self.learning_rate = np.clip(self.learning_rate, 1e-5, 1e-2)
        
        # æ›´æ–°ä¼˜åŒ–å™¨å­¦ä¹ ç‡
        for param_group in self.optimizer.param_groups:
            param_group['lr'] = self.learning_rate
        
        print(f"[Agent {self.id}] å‚æ•°æ‰°åŠ¨å®Œæˆ: {self.mcts_params}, lr={self.learning_rate:.2e}")


class PBTTrainer:
    """Population-Based Training ä¸»è®­ç»ƒå™¨"""
    
    def __init__(self, args):
        self.args = args
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"\n{'='*60}")
        print(f"Population-Based Training for MCTS Program Synthesis")
        print(f"{'='*60}")
        print(f"Agentæ•°é‡: {args.n_agents}")
        print(f"PBTé—´éš”: æ¯{args.pbt_interval}è½®è¯„ä¼°ä¸€æ¬¡")
        print(f"æ·˜æ±°ç‡: {args.exploit_threshold*100:.0f}%")
        print(f"è®¾å¤‡: {self.device}")
        print(f"{'='*60}\n")
        
        # åˆå§‹åŒ–evaluatorï¼ˆæ‰€æœ‰agentå…±äº«ï¼‰
        self.evaluator = BatchEvaluator(
            trajectory_type=args.traj,
            duration=args.duration,
            num_envs=args.isaac_num_envs,
            device='cuda:0',
            headless=True,
            reward_profile=args.reward_profile
        )
        
        # åˆ›å»ºå…±äº«çš„GNNç¼–ç å™¨ï¼ˆå¯é€‰ï¼ŒèŠ‚çœæ˜¾å­˜ï¼‰
        if args.shared_encoder:
            print("[PBT] ä½¿ç”¨å…±äº«GNNç¼–ç å™¨æ¨¡å¼ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰")
            self.shared_encoder = self._create_shared_encoder()
        else:
            print("[PBT] ä½¿ç”¨ç‹¬ç«‹GNNæ¨¡å‹æ¨¡å¼")
            self.shared_encoder = None
        
        # åˆå§‹åŒ–æ‰€æœ‰agents
        self.agents: List[PBTAgent] = []
        for i in range(args.n_agents):
            agent = PBTAgent(i, args, self.device, self.shared_encoder)
            self.agents.append(agent)
        
        # PBTç»Ÿè®¡
        self.global_best_reward = -float('inf')
        self.global_best_program = None
        self.global_best_agent_id = -1
        
        # æ¯ä¸ªagentçš„è®­ç»ƒå™¨ï¼ˆå¤ç”¨OnlineTrainerçš„MCTSé€»è¾‘ï¼‰
        self._setup_agent_trainers()
    
    def _create_shared_encoder(self):
        """åˆ›å»ºå…±äº«çš„GNNç¼–ç å™¨"""
        model = create_gnn_policy_value_net_v2(
            node_feat_dim=self.args.node_feat_dim if hasattr(self.args, 'node_feat_dim') else 16,
            hidden_channels=self.args.hidden_channels if hasattr(self.args, 'hidden_channels') else 128,
            num_gnn_layers=self.args.num_gnn_layers if hasattr(self.args, 'num_gnn_layers') else 3,
            n_edit_types=len(EDIT_TYPES),
            dropout=0.1
        ).to(self.device)
        
        return model.gnn_encoder  # åªè¿”å›ç¼–ç å™¨éƒ¨åˆ†
    
    def _setup_agent_trainers(self):
        """ä¸ºæ¯ä¸ªagentåˆ›å»ºè®­ç»ƒå™¨å®ä¾‹ï¼ˆå¤ç”¨OnlineTrainerçš„MCTSé€»è¾‘ï¼‰"""
        for agent in self.agents:
            # åˆ›å»ºä¸€ä¸ªè½»é‡çº§çš„trainer wrapper
            # è¿™é‡Œæˆ‘ä»¬å¤ç”¨OnlineTrainerçš„mcts_searchæ–¹æ³•
            # ä½†ä½¿ç”¨agentè‡ªå·±çš„å‚æ•°
            agent.trainer = self._create_agent_trainer_wrapper(agent)
    
    def _create_agent_trainer_wrapper(self, agent: PBTAgent):
        """åˆ›å»ºagentçš„è®­ç»ƒå™¨wrapper"""
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„wrapperï¼Œå¤ç”¨OnlineTrainerçš„æ ¸å¿ƒæ–¹æ³•
        class AgentTrainerWrapper:
            def __init__(self, parent_trainer, agent):
                self.parent = parent_trainer
                self.agent = agent
                self.args = agent.args
                self.device = agent.device
                self.evaluator = parent_trainer.evaluator
                
                # ä½¿ç”¨agentçš„æ¨¡å‹
                if agent.nn_model is not None:
                    self.nn_model = agent.nn_model
                else:
                    # å…±äº«ç¼–ç å™¨æ¨¡å¼ï¼šåˆ›å»ºä¸´æ—¶çš„forward wrapper
                    self.nn_model = lambda graph: agent.forward(graph)
                
                # ä½¿ç”¨agentçš„MCTSå‚æ•°
                self._update_mcts_params()
            
            def _update_mcts_params(self):
                """ä»agentåŒæ­¥MCTSå‚æ•°"""
                params = self.agent.mcts_params
                self._puct_c = params['puct_c']
                self._exploration_weight = params['exploration_weight']
                self._root_dirichlet_eps = params['dirichlet_eps']
                self._root_dirichlet_alpha = params['dirichlet_alpha']
                self._policy_temperature = params['temperature']
                self._max_depth = 12  # å›ºå®š
            
            def mcts_search(self, *args, **kwargs):
                """è°ƒç”¨çˆ¶trainerçš„mcts_searchï¼ˆä½†ä½¿ç”¨agentçš„å‚æ•°ï¼‰"""
                # è¿™é‡Œéœ€è¦å¤ç”¨OnlineTrainer.mcts_searchçš„å®Œæ•´é€»è¾‘
                # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æš‚æ—¶æ ‡è®°ä¸ºTODO
                # å®é™…å®ç°ä¸­éœ€è¦å®Œæ•´å¤åˆ¶æˆ–é‡æ„mcts_search
                pass  # TODO: å®ç°
        
        return AgentTrainerWrapper(self, agent)
    
    def train(self):
        """PBTä¸»è®­ç»ƒå¾ªç¯"""
        for global_iter in range(self.args.total_iters):
            iter_start = time.time()
            
            print(f"\n{'='*60}")
            print(f"Iteration {global_iter + 1} / {self.args.total_iters}")
            print(f"{'='*60}")
            
            # 1. æ‰€æœ‰agentå¹¶è¡Œè®­ç»ƒä¸€æ­¥
            agent_rewards = self._parallel_train_step(global_iter)
            
            # 2. æ›´æ–°å…¨å±€æœ€ä½³
            self._update_global_best(agent_rewards, global_iter)
            
            # 3. PBTè°ƒåº¦ï¼ˆå‘¨æœŸæ€§ï¼‰
            if (global_iter + 1) % self.args.pbt_interval == 0:
                self._pbt_exploit_explore()
            
            # 4. ç»Ÿè®¡è¾“å‡º
            iter_time = time.time() - iter_start
            self._print_statistics(global_iter, agent_rewards, iter_time)
            
            # 5. ä¿å­˜checkpoint
            if (global_iter + 1) % self.args.save_freq == 0:
                self._save_checkpoint(global_iter)
        
        # æœ€ç»ˆä¿å­˜
        self._save_final_results()
    
    def _parallel_train_step(self, iteration: int) -> List[float]:
        """æ‰€æœ‰agentå¹¶è¡Œè®­ç»ƒä¸€æ­¥ï¼Œè¿”å›æ¯ä¸ªagentçš„å¥–åŠ±"""
        agent_rewards = []
        
        # ä¸ºæ¯ä¸ªagentæ‰§è¡Œå®Œæ•´çš„è®­ç»ƒæ­¥éª¤
        for agent_idx, agent in enumerate(self.agents):
            try:
                # 1. MCTSæœç´¢ + ç”Ÿæˆæ–°ç¨‹åº
                current_program = agent.best_program if agent.best_program is not None else self._generate_random_program()
                
                # åº”ç”¨agentçš„MCTSå‚æ•°
                self._apply_agent_mcts_params(agent)
                
                # è¿è¡ŒMCTSæœç´¢ï¼ˆä½¿ç”¨agentçš„æ¨¡å‹ï¼‰
                children, visit_counts = self._mcts_search_for_agent(
                    agent, current_program, 
                    num_simulations=agent.mcts_params['simulations']
                )
                
                # 2. é€‰æ‹©ä¸‹ä¸€ä¸ªç¨‹åºï¼ˆæ ¹æ®è®¿é—®è®¡æ•°ï¼‰
                if children and len(visit_counts) > 0:
                    # ä½¿ç”¨temperatureé‡‡æ ·
                    next_program = self._select_next_program(children, visit_counts, agent.mcts_params['temperature'])
                else:
                    next_program = current_program
                    print(f"[Agent {agent.id}] è­¦å‘Š: MCTSæœªç”Ÿæˆå­èŠ‚ç‚¹")
                
                # 3. è¯„ä¼°ç¨‹åº
                reward = self.evaluator.evaluate_single(next_program)
                
                # 4. æ·»åŠ è®­ç»ƒæ ·æœ¬åˆ°replay buffer
                if children and len(visit_counts) > 0:
                    self._add_training_sample(agent, current_program, visit_counts, reward)
                
                # 5. æ›´æ–°æ€§èƒ½å†å²
                agent.performance_history.append(reward)
                agent.total_iterations += 1
                
                if reward > agent.best_reward:
                    agent.best_reward = reward
                    agent.best_program = next_program
                    print(f"[Agent {agent.id}] ğŸ‰ æ–°æœ€ä½³å¥–åŠ±: {reward:.4f}")
                
                # 6. å‘¨æœŸæ€§æ›´æ–°NN
                if (iteration + 1) % self.args.update_freq == 0 and len(agent.replay_buffer) >= 8:
                    self._train_agent_nn(agent)
                
                agent_rewards.append(reward)
                
            except Exception as e:
                print(f"[Agent {agent.id}] é”™è¯¯: {e}")
                import traceback
                traceback.print_exc()
                # å¤±è´¥æ—¶ä½¿ç”¨å½“å‰æœ€ä½³å¥–åŠ±æˆ–æœ€ä½åˆ†
                reward = agent.best_reward if agent.best_reward > -float('inf') else -10.0
                agent_rewards.append(reward)
        
        return agent_rewards
    
    def _update_global_best(self, agent_rewards: List[float], iteration: int):
        """æ›´æ–°å…¨å±€æœ€ä½³agent"""
        for i, (agent, reward) in enumerate(zip(self.agents, agent_rewards)):
            if reward > self.global_best_reward:
                self.global_best_reward = reward
                self.global_best_program = agent.best_program
                self.global_best_agent_id = agent.id
                print(f"[Iter {iteration+1}] ğŸ‰ æ–°å…¨å±€æœ€ä½³ï¼Agent {agent.id}, Reward: {reward:.4f}")
    
    def _pbt_exploit_explore(self):
        """PBTçš„æ ¸å¿ƒï¼šExploit & Explore"""
        print(f"\n{'='*60}")
        print(f"PBTè°ƒåº¦ï¼šExploit & Explore")
        print(f"{'='*60}")
        
        # 1. è®¡ç®—æ¯ä¸ªagentçš„æ€§èƒ½ï¼ˆæœ€è¿‘10è½®å¹³å‡ï¼‰
        performances = []
        for agent in self.agents:
            if len(agent.performance_history) > 0:
                recent = list(agent.performance_history)[-10:]
                perf = np.mean(recent)
            else:
                perf = -float('inf')
            performances.append((agent.id, perf, agent))
        
        # 2. æ’åº
        performances.sort(key=lambda x: x[1], reverse=True)
        
        # æ‰“å°æ’å
        print("\nAgentæ€§èƒ½æ’å:")
        for rank, (agent_id, perf, agent) in enumerate(performances, 1):
            marker = "ğŸ†" if rank <= 3 else ("â­" if rank <= len(self.agents)//2 else "")
            print(f"  {rank}. Agent {agent_id}: {perf:.4f} {marker}")
        
        # 3. æ·˜æ±°ä¸‹ä½20%ï¼Œå¤åˆ¶ä¸Šä½agent
        n_exploit = max(1, int(self.args.n_agents * self.args.exploit_threshold))
        
        top_agents = [agent for _, _, agent in performances[:n_exploit]]
        bottom_agents = [agent for _, _, agent in performances[-n_exploit:]]
        
        print(f"\næ·˜æ±°ä¸‹ä½{n_exploit}ä¸ªagentï¼Œå¤åˆ¶ä¸Šä½agent:")
        for weak_agent in bottom_agents:
            # éšæœºé€‰ä¸€ä¸ªå¼ºagent
            strong_agent = random.choice(top_agents)
            
            print(f"  ğŸ”„ Agent {weak_agent.id} (â­{weak_agent.best_reward:.2f}) "
                  f"å¤åˆ¶ Agent {strong_agent.id} (â­{strong_agent.best_reward:.2f})")
            
            # å¤åˆ¶æƒé‡
            weak_agent.copy_from(strong_agent)
            
            # æ‰°åŠ¨å‚æ•°
            weak_agent.perturb_params(perturb_factors=(0.8, 1.2))
        
        print(f"{'='*60}\n")
    
    def _print_statistics(self, iteration: int, agent_rewards: List[float], iter_time: float):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        mean_reward = np.mean(agent_rewards)
        max_reward = np.max(agent_rewards)
        min_reward = np.min(agent_rewards)
        std_reward = np.std(agent_rewards)
        
        print(f"\nç»Ÿè®¡:")
        print(f"  å¹³å‡å¥–åŠ±: {mean_reward:.4f}")
        print(f"  æœ€å¤§å¥–åŠ±: {max_reward:.4f}")
        print(f"  æœ€å°å¥–åŠ±: {min_reward:.4f}")
        print(f"  æ ‡å‡†å·®: {std_reward:.4f}")
        print(f"  å…¨å±€æœ€ä½³: {self.global_best_reward:.4f} (Agent {self.global_best_agent_id})")
        print(f"  ç”¨æ—¶: {iter_time:.2f}s")
    
    def _save_checkpoint(self, iteration: int):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'iteration': iteration,
            'global_best_reward': self.global_best_reward,
            'global_best_agent_id': self.global_best_agent_id,
            'agents': []
        }
        
        for agent in self.agents:
            agent_data = {
                'id': agent.id,
                'mcts_params': agent.mcts_params,
                'learning_rate': agent.learning_rate,
                'best_reward': agent.best_reward,
                'model_state': agent.get_model_state_dict()
            }
            checkpoint['agents'].append(agent_data)
        
        save_path = self.args.save_path.replace('.json', f'_pbt_iter{iteration+1}.pt')
        torch.save(checkpoint, save_path)
        print(f"  ğŸ’¾ Checkpointå·²ä¿å­˜: {save_path}")
    
    def _save_final_results(self):
        """ä¿å­˜æœ€ç»ˆç»“æœ"""
        print(f"\n{'='*60}")
        print(f"è®­ç»ƒå®Œæˆï¼")
        print(f"å…¨å±€æœ€ä½³å¥–åŠ±: {self.global_best_reward:.4f}")
        print(f"æ¥è‡ªAgent {self.global_best_agent_id}")
        print(f"{'='*60}\n")
        
        # ä¿å­˜æœ€ä½³ç¨‹åº
        if self.global_best_program is not None:
            final_path = self.args.save_path.replace('.json', '_pbt_final.json')
            save_program_json(self.global_best_program, final_path)
            print(f"æœ€ä½³ç¨‹åºå·²ä¿å­˜: {final_path}")
    
    # ============================================================
    # è¾…åŠ©æ–¹æ³•ï¼šagentè®­ç»ƒå¾ªç¯ç›¸å…³
    # ============================================================
    
    def _generate_random_program(self):
        """ç”Ÿæˆéšæœºåˆå§‹ç¨‹åº"""
        from core.dsl import Rule
        # ç®€å•çš„PIDæ§åˆ¶å™¨åˆå§‹åŒ–
        return [
            Rule(op='set', var='u_x', expr={'type': 'const', 'value': 0.0}),
            Rule(op='set', var='u_y', expr={'type': 'const', 'value': 0.0}),
            Rule(op='set', var='u_z', expr={'type': 'const', 'value': 0.0}),
        ]
    
    def _apply_agent_mcts_params(self, agent: PBTAgent):
        """åº”ç”¨agentçš„MCTSå‚æ•°åˆ°å…¨å±€æœç´¢é…ç½®"""
        # è¿™äº›å‚æ•°ä¼šåœ¨mcts_searchä¸­ä½¿ç”¨
        self._current_mcts_params = agent.mcts_params
    
    def _mcts_search_for_agent(self, agent: PBTAgent, root_program, num_simulations: int):
        """ä¸ºagentæ‰§è¡ŒMCTSæœç´¢"""
        from mcts_training.mcts import MCTS_Agent, MCTSNode
        
        # åˆ›å»ºMCTSå®ä¾‹
        mcts = MCTS_Agent(
            evaluator=self.evaluator,
            exploration_weight=agent.mcts_params['exploration_weight'],
            max_depth=12  # å›ºå®š
        )
        
        # åˆ›å»ºæ ¹èŠ‚ç‚¹
        root = MCTSNode(program=root_program, parent=None, depth=0)
        
        # MCTSæœç´¢å¾ªç¯ï¼ˆç®€åŒ–ç‰ˆï¼‰
        for sim_idx in range(num_simulations):
            node = root
            path = [node]
            
            # Selection: å‘ä¸‹é€‰æ‹©åˆ°å¶å­èŠ‚ç‚¹
            while node.children and not node.is_terminal():
                node = self._select_child_puct(node, agent)
                path.append(node)
            
            # Expansion: å¦‚æœæœªå®Œå…¨æ‰©å±•ï¼Œæ‰©å±•ä¸€ä¸ªæ–°å­èŠ‚ç‚¹
            if not node.is_fully_expanded() and not node.is_terminal():
                child = self._expand_node(node, mcts, agent)
                if child:
                    path.append(child)
                    node = child
            
            # Simulation: è¯„ä¼°å¶å­èŠ‚ç‚¹
            reward = self.evaluator.evaluate_single(node.program)
            
            # Backpropagation: å›ä¼ å¥–åŠ±
            for n in reversed(path):
                n.visits += 1
                n.value_sum += reward
        
        # è¿”å›æ ¹èŠ‚ç‚¹çš„childrenå’Œè®¿é—®è®¡æ•°
        if root.children:
            children = root.children
            visit_counts = [child.visits for child in children]
            return children, visit_counts
        else:
            return [], []
    
    def _select_child_puct(self, node: 'MCTSNode', agent: PBTAgent) -> 'MCTSNode':
        """PUCTé€‰æ‹©ï¼ˆä½¿ç”¨agentçš„å‚æ•°ï¼‰"""
        import math
        
        best_score = -float('inf')
        best_child = None
        
        puct_c = agent.mcts_params['puct_c']
        
        for child in node.children:
            if child.visits == 0:
                return child  # ä¼˜å…ˆé€‰æ‹©æœªè®¿é—®çš„
            
            # PUCTå…¬å¼
            q_value = child.value_sum / child.visits
            u_value = puct_c * math.sqrt(node.visits) / (1 + child.visits)
            
            # è·å–priorï¼ˆå¦‚æœæœ‰ï¼‰
            prior = getattr(child, 'prior', 1.0 / len(node.children))
            u_value *= prior
            
            score = q_value + u_value
            
            if score > best_score:
                best_score = score
                best_child = child
        
        return best_child if best_child else node.children[0]
    
    def _expand_node(self, node: 'MCTSNode', mcts: 'MCTS_Agent', agent: PBTAgent) -> Optional['MCTSNode']:
        """æ‰©å±•èŠ‚ç‚¹ï¼ˆåˆ›å»ºæ–°å­èŠ‚ç‚¹ï¼‰"""
        from mcts_training.mcts import MCTSNode
        from core.dsl import mutate_program
        
        try:
            # ç”Ÿæˆå˜å¼‚ç¨‹åº
            mutated_program = mutate_program(node.program)
            
            # åˆ›å»ºå­èŠ‚ç‚¹
            child = MCTSNode(program=mutated_program, parent=node, depth=node.depth + 1)
            
            # ä½¿ç”¨GNNè·å–prior
            with torch.no_grad():
                graph = ast_to_pyg_graph(mutated_program)
                batch_graph = PyGBatch.from_data_list([graph]).to(self.device)
                policy_logits, _, _ = agent.forward(batch_graph)
                policy_probs = torch.softmax(policy_logits, dim=-1)
                # è¿™é‡Œç®€åŒ–ï¼šä½¿ç”¨ç¬¬ä¸€ä¸ªedit typeçš„æ¦‚ç‡ä½œä¸ºprior
                prior = float(policy_probs[0][0].item())
            
            child.prior = prior
            node.children.append(child)
            
            return child
        except Exception as e:
            print(f"[Agent {agent.id}] æ‰©å±•èŠ‚ç‚¹å¤±è´¥: {e}")
            return None
    
    def _select_next_program(self, children, visit_counts, temperature: float):
        """æ ¹æ®è®¿é—®è®¡æ•°å’Œæ¸©åº¦é€‰æ‹©ä¸‹ä¸€ä¸ªç¨‹åº"""
        import numpy as np
        
        if temperature < 1e-8:
            # è´ªå¿ƒé€‰æ‹©
            best_idx = np.argmax(visit_counts)
            return children[best_idx].program
        else:
            # æ¸©åº¦é‡‡æ ·
            counts = np.array(visit_counts, dtype=np.float64)
            scaled = counts ** (1.0 / max(1e-6, temperature))
            probs = scaled / max(1e-12, scaled.sum())
            choice = int(np.random.choice(len(children), p=probs))
            return children[choice].program
    
    def _add_training_sample(self, agent: PBTAgent, program, visit_counts, reward):
        """æ·»åŠ è®­ç»ƒæ ·æœ¬åˆ°agentçš„replay buffer"""
        import torch
        
        try:
            # æ„å»ºpolicy targetï¼ˆMCTSè®¿é—®åˆ†å¸ƒï¼‰
            visit_counts = np.array(visit_counts, dtype=np.float64)
            policy_target = visit_counts / max(1.0, visit_counts.sum())
            
            # ç¡®ä¿targeté•¿åº¦ä¸EDIT_TYPESä¸€è‡´
            full_target = np.zeros(len(EDIT_TYPES), dtype=np.float32)
            for i in range(min(len(policy_target), len(EDIT_TYPES))):
                full_target[i] = policy_target[i]
            
            # å½’ä¸€åŒ–
            if full_target.sum() > 0:
                full_target = full_target / full_target.sum()
            else:
                full_target = np.ones(len(EDIT_TYPES), dtype=np.float32) / len(EDIT_TYPES)
            
            # æ„å»ºæ ·æœ¬
            sample = {
                'graph': ast_to_pyg_graph(program),
                'policy_target': torch.tensor(full_target, dtype=torch.float32)
            }
            
            agent.replay_buffer.push(sample)
            
        except Exception as e:
            print(f"[Agent {agent.id}] æ·»åŠ æ ·æœ¬å¤±è´¥: {e}")
    
    def _train_agent_nn(self, agent: PBTAgent):
        """è®­ç»ƒagentçš„ç¥ç»ç½‘ç»œ"""
        if len(agent.replay_buffer) < 8:
            return
        
        try:
            total_loss = 0.0
            for _ in range(self.args.train_steps_per_update):
                # é‡‡æ ·batch
                actual_batch_size = min(self.args.batch_size, len(agent.replay_buffer))
                batch = agent.replay_buffer.sample(actual_batch_size)
                
                # æ„å»ºtensor
                graph_list = [s['graph'] for s in batch]
                batch_graph = PyGBatch.from_data_list(graph_list).to(self.device)
                policy_targets = torch.stack([s['policy_target'] for s in batch]).to(self.device)
                
                # å‰å‘ä¼ æ’­
                policy_logits, _, _ = agent.forward(batch_graph)
                
                # ç­–ç•¥æŸå¤±
                policy_loss = -(policy_targets * torch.nn.functional.log_softmax(policy_logits, dim=-1)).sum(dim=-1).mean()
                
                # ç†µæ­£åˆ™
                policy_probs = torch.nn.functional.softmax(policy_logits, dim=-1)
                policy_entropy = (-(policy_probs.clamp(min=1e-12) * policy_probs.clamp(min=1e-12).log()).sum(dim=-1)).mean()
                
                loss = policy_loss - 0.01 * policy_entropy
                
                # åå‘ä¼ æ’­
                agent.optimizer.zero_grad()
                loss.backward()
                torch.nn.utils.clip_grad_norm_(
                    list(agent.shared_encoder.parameters()) + list(agent.policy_head.parameters()) 
                    if agent.shared_encoder is not None 
                    else agent.nn_model.parameters(),
                    1.0
                )
                agent.optimizer.step()
                
                total_loss += loss.item()
            
            avg_loss = total_loss / self.args.train_steps_per_update
            if agent.total_iterations % 10 == 0:  # æ¯10è½®æ‰“å°ä¸€æ¬¡
                print(f"[Agent {agent.id}] NNæ›´æ–°: loss={avg_loss:.4f}")
            
        except Exception as e:
            print(f"[Agent {agent.id}] NNè®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()


def parse_args():
    p = argparse.ArgumentParser(description='Population-Based Training for MCTS Program Synthesis')
    
    # PBTå‚æ•°
    p.add_argument('--n-agents', type=int, default=16, help='Agentæ•°é‡')
    p.add_argument('--pbt-interval', type=int, default=50, help='PBTè°ƒåº¦é—´éš”ï¼ˆè½®æ•°ï¼‰')
    p.add_argument('--exploit-threshold', type=float, default=0.25, help='æ·˜æ±°æ¯”ä¾‹ï¼ˆ0-1ï¼‰')
    p.add_argument('--shared-encoder', action='store_true', help='ä½¿ç”¨å…±äº«GNNç¼–ç å™¨ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰')
    
    # è®­ç»ƒå‚æ•°
    p.add_argument('--total-iters', type=int, default=5000, help='æ€»è¿­ä»£æ•°')
    p.add_argument('--update-freq', type=int, default=50, help='NNæ›´æ–°é¢‘ç‡')
    p.add_argument('--train-steps-per-update', type=int, default=10, help='æ¯æ¬¡æ›´æ–°çš„è®­ç»ƒæ­¥æ•°')
    p.add_argument('--batch-size', type=int, default=64, help='æ‰¹é‡å¤§å°ï¼ˆé™ä½ä»¥é€‚åº”å¤šagentï¼‰')
    p.add_argument('--replay-capacity', type=int, default=20000, help='æ¯ä¸ªagentçš„replay bufferå®¹é‡')
    
    # GNNå‚æ•°
    p.add_argument('--node-feat-dim', type=int, default=16, help='èŠ‚ç‚¹ç‰¹å¾ç»´åº¦')
    p.add_argument('--hidden-channels', type=int, default=128, help='éšè—å±‚ç»´åº¦')
    p.add_argument('--num-gnn-layers', type=int, default=3, help='GNNå±‚æ•°')
    p.add_argument('--learning-rate', type=float, default=1e-3, help='åˆå§‹å­¦ä¹ ç‡ï¼ˆPBTä¼šè°ƒæ•´ï¼‰')
    
    # ä»¿çœŸå‚æ•°
    p.add_argument('--traj', type=str, default='figure8', choices=['hover', 'figure8', 'circle', 'helix'])
    p.add_argument('--duration', type=int, default=10, help='ä»¿çœŸæ—¶é•¿ï¼ˆç§’ï¼‰')
    p.add_argument('--isaac-num-envs', type=int, default=512, help='Isaac Gymå¹¶è¡Œç¯å¢ƒæ•°')
    p.add_argument('--reward-profile', type=str, default='control_law_discovery', 
                   choices=['default', 'control_law_discovery', 'smooth_control', 'balanced_smooth'])
    
    # ä¿å­˜å‚æ•°
    p.add_argument('--save-path', type=str, default='results/pbt_best_program.json')
    p.add_argument('--save-freq', type=int, default=200, help='Checkpointä¿å­˜é¢‘ç‡')
    
    return p.parse_args()


if __name__ == '__main__':
    args = parse_args()
    
    # åˆ›å»ºPBTè®­ç»ƒå™¨
    pbt_trainer = PBTTrainer(args)
    
    # å¼€å§‹è®­ç»ƒ
    pbt_trainer.train()
